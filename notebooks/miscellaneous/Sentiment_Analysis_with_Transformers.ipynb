{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "584edc8a-0561-43a3-b201-497268d4a94a",
   "metadata": {},
   "source": [
    "### Sentiment Analysis with Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1bc2711-ac69-4a7a-915d-9946a4ffc6b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%env PIP_DISABLE_PIP_VERSION_CHECK=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3fa6fa8-8236-4b7a-955e-7c27e507ae78",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q -U watermark\n",
    "!pip install -q spacy\n",
    "!pip install -q tensorflow\n",
    "!pip install -q nltk transformers\n",
    "!pip install -q keras tf-keras keras-preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f4b54c8-e00e-4a17-b1b8-e2d8aa82fbb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%env TF_CPP_MIN_LOG_LEVEL=3\n",
    "%env TF_ENABLE_ONEDNN_OPTS=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8470eebb-f2ab-46dc-b8e7-5ecd7cbe4c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Imports\n",
    "import os\n",
    "import math\n",
    "import nltk\n",
    "import spacy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import transformers\n",
    "from tokenizers import BertWordPieceTokenizer\n",
    "from tqdm import tqdm\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.models import load_model\n",
    "from keras_preprocessing.text import Tokenizer\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "from keras.metrics import Precision, Recall, AUC\n",
    "from keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional\n",
    "from keras.callbacks import EarlyStopping, LearningRateScheduler, CallbackList, ReduceLROnPlateau\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.regularizers import l1_l2\n",
    "from keras.saving import register_keras_serializable\n",
    "from tensorflow.keras.layers import Layer, Dense\n",
    "from transformers import TFDistilBertModel, DistilBertConfig\n",
    "from tensorflow.keras.metrics import Precision, Recall, AUC\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.filterwarnings('error', category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f67abc4f-f87f-4ce1-a70d-1b48356a5aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext watermark\n",
    "%watermark -a \"shortthirdman\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b923e7f-676d-4e4d-8179-fb73fb657429",
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.environ['TF_ENABLE_ONEDNN_OPTS'] != '0':\n",
    "    print(\"TensorFlow has not been correctly\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a13a5505-f3a9-4bce-ac56-6fe57634a01f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.a Loading Training Data\n",
    "training_data = pd.read_csv('../../data/training_data.txt', header=None, delimiter=';')\n",
    "\n",
    "# 2.b Loading Test Data\n",
    "test_data = pd.read_csv('../../data/test_data.txt', header=None, delimiter=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d787fd01-dbb6-43da-987a-d25e0433e54d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Adjusting Column Names\n",
    "training_data = training_data.rename(columns={0: 'text', 1: 'sentiment'})\n",
    "test_data = test_data.rename(columns={0: 'text', 1: 'sentiment'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be277176-8158-4018-9174-3c7786bde76b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Checking Dataset Shape\n",
    "training_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76a6f146-513f-4e7a-b69b-d226d5612d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Checking Test Dataset Shape\n",
    "test_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9ba171c-d623-4cba-90c1-9aa5b2b55dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Training Data Sample\n",
    "training_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52f45ac5-c81d-488a-aa32-5a88966f44f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Sentiments in Training Data\n",
    "training_data['sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a205f18b-95de-4a09-8b60-d166690df426",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. Sentiments in Test Data\n",
    "test_data['sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceb94c29-303b-4421-a798-887411250936",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m spacy download en_core_web_md -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb3b355b-c30b-4685-adbd-219cd137fba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9. Load SpaCy Model\n",
    "nlp = spacy.load('en_core_web_md')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b19982e4-b3ab-4301-9056-f5259b60715d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10. Definition of the 'preprocess_text' Function, Which Takes a Text as a Parameter\n",
    "def preprocess_text(text):\n",
    "\n",
    "    # 10.a Process the text using the SpaCy model\n",
    "    doc = nlp(text)\n",
    "\n",
    "    # 10.b Create a list of lemmatized tokens, converted to lowercase, stripped of whitespace,\n",
    "    # excluding stopwords\n",
    "    tokens = [token.lemma_.lower().strip() for token in doc if not token.is_stop]\n",
    "\n",
    "    # 10.c Return the processed tokens as a single string, joined with spaces\n",
    "    return ' '.join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce8c4043-3d49-45d2-81a3-618ceaa4f61f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# 11. Apply the Preprocessing Function to Training Data\n",
    "training_data['processed_text'] = training_data['text'].apply(preprocess_text)\n",
    "\n",
    "# 12. Apply the Preprocessing Function to Test Data\n",
    "test_data['processed_text'] = test_data['text'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8b5086f-9d2f-47ac-a569-4469058f5eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 13. Data Sample\n",
    "training_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a14f95a8-74d1-4b20-9551-4758d25ca31f",
   "metadata": {},
   "source": [
    "#### Model Implementation: Vectorization with TF-IDF\n",
    "\n",
    "TF-IDF (Term Frequency-Inverse Document Frequency) is a technique that has been used for a long time in natural language processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9619a50e-5b69-4ae5-86d8-9cac81be0b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 14. Create the Vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.95, min_df=2, stop_words='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f31e3df4-8259-4349-bee7-11bd860222c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 15. Apply the Vectorizer\n",
    "train_tfidf = tfidf_vectorizer.fit_transform(training_data['processed_text'])\n",
    "test_tfidf = tfidf_vectorizer.transform(test_data['processed_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c6141ec-1989-469c-b1a7-b0117483f78b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 16. Check Shape of Training Data TF-IDF Matrix\n",
    "train_tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "673be088-cdc9-40a6-b199-c6796164fb6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 17. Check Type of Training Data TF-IDF Matrix\n",
    "type(train_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f71aa3c3-c750-4bcc-96f5-3e2148c775c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 18. Convert Input Data (Text) to Array\n",
    "X_train_array = train_tfidf.toarray()\n",
    "X_test_array = test_tfidf.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f68aabb0-5c4a-4e18-8cfa-225bd397489c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 19. Create the Label Encoder\n",
    "label_encoder_v1 = LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd3d7982-e918-47d8-9afb-640596132815",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 20. Fit and Transform the Target Variable in Training Data\n",
    "y_train_le = label_encoder_v1.fit_transform(training_data['sentiment'])\n",
    "\n",
    "# 21. Transform the Target Variable in Test Data\n",
    "y_test_le = label_encoder_v1.transform(test_data['sentiment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa7ca885-7976-4b50-84fd-0e37a0062c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 22. Compute Class Weights\n",
    "class_weights = compute_class_weight('balanced', classes=np.unique(y_train_le), y=y_train_le)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "263dafd8-3146-4039-ad5e-497b5d1d4fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 24. Split Data into Training and Validation Sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_train_array,\n",
    "    y_train_le,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=y_train_le\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3f2e5f1-0a93-49b9-b749-cb15ab22eae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 25. Convert Target Variable to Categorical Type\n",
    "y_train_encoded = to_categorical(y_train)\n",
    "y_test_encoded = to_categorical(y_test_le)\n",
    "y_val_encoded = to_categorical(y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36b972ad-f623-49e0-8609-ac415c1bbdf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 26. Check Shape of Encoded Target Variables\n",
    "y_train_encoded.shape, y_test_encoded.shape, y_val_encoded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ca24c50-67dc-490d-89e1-5a3d616f752c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 27. Create the Model\n",
    "\n",
    "# 27.a Initialize a sequential model. Sequential models are a linear stack of layers.\n",
    "model_v1 = Sequential()\n",
    "\n",
    "# 27.b Add the first dense (fully-connected) layer to the model\n",
    "model_v1.add(Dense(\n",
    "    4096,\n",
    "    activation='selu',  # Use the SELU (Scaled Exponential Linear Unit) activation function\n",
    "    kernel_initializer='lecun_normal',  # Initialize weights with LeCun normal distribution\n",
    "    input_shape=(X_train.shape[1],),  # Define input shape based on the number of features in X_train\n",
    "    kernel_regularizer=tf.keras.regularizers.l2(0.01)  # Apply L2 regularization to reduce overfitting\n",
    "))\n",
    "\n",
    "# 27.c Add the second dense layer\n",
    "model_v1.add(Dense(\n",
    "    2048,\n",
    "    activation='selu',\n",
    "    kernel_initializer='lecun_normal',\n",
    "    kernel_regularizer=tf.keras.regularizers.l2(0.01)\n",
    "))\n",
    "\n",
    "# 27.d Add the third dense layer\n",
    "model_v1.add(Dense(\n",
    "    1024,\n",
    "    activation='selu',\n",
    "    kernel_initializer='lecun_normal',\n",
    "    kernel_regularizer=tf.keras.regularizers.l2(0.1)\n",
    "))\n",
    "\n",
    "# 27.e Add the fourth dense layer\n",
    "# Layer with 64 neurons and SELU activation\n",
    "model_v1.add(Dense(64, activation='selu'))\n",
    "\n",
    "# 27.f Add the output layer\n",
    "# Output layer with 6 neurons and softmax activation for multi-class classification\n",
    "model_v1.add(Dense(6, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72470096-60ca-42ed-8ebb-872e0a600031",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 28. Assign Specific Weights to the Bias Vector of the Model's Last Layer\n",
    "model_v1.layers[-1].bias.assign(class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3918d9af-6102-4b0e-9f8d-218a20e639d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 29. Compile the Model\n",
    "\n",
    "# 29.a Define the optimizer as 'Adam'.\n",
    "# Adam is an optimization algorithm used as an alternative to the classic stochastic gradient descent\n",
    "# procedure, updating network weights iteratively based on training data.\n",
    "\n",
    "# 29.b Set the loss function to 'categorical_crossentropy'.\n",
    "# This is suitable for multi-class classification problems where labels are provided in one-hot encoded format.\n",
    "\n",
    "# 29.c Specify the evaluation metrics for the model as 'accuracy', along with Precision, Recall, and AUC.\n",
    "# Accuracy is a common metric to evaluate classification model performance.\n",
    "model_v1.compile(\n",
    "    optimizer='Adam',\n",
    "    loss=tf.losses.categorical_crossentropy,\n",
    "    metrics=['accuracy', Precision(), Recall(), AUC()]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c84d990e-0f68-48a4-9978-cdc646072343",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 30. Display Model Summary\n",
    "model_v1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "592a76e7-99b3-4f59-9046-900f1d6ca1d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 31. Learning Rate Scheduler Function\n",
    "def step_decay(epoch):\n",
    "    \n",
    "    # 31.a Initial learning rate\n",
    "    initial_lrate = 0.001\n",
    "    \n",
    "    # 31.b Drop factor for learning rate decay\n",
    "    drop = 0.5\n",
    "    \n",
    "    # 31.c Number of epochs after which learning rate is reduced\n",
    "    epochs_drop = 10.0\n",
    "    \n",
    "    # 31.d Calculate the updated learning rate\n",
    "    lrate = initial_lrate * math.pow(drop, math.floor((1 + epoch) / epochs_drop))\n",
    "    \n",
    "    return lrate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8d5523b-9991-41a6-b50b-77ef30ddc62d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 32. Learning Rate Scheduler\n",
    "lr_scheduler = LearningRateScheduler(step_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dd71ca1-92f6-4b18-8e03-61a2bf299602",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 33. Early Stopping\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',  # Monitor the validation loss\n",
    "    restore_best_weights=True,  # Restore the model weights from the epoch with the best validation loss\n",
    "    patience=3  # Stop training after 3 epochs with no improvement\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "322f80c9-34e6-4c7b-9e61-970e11d7a7a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 34. Hyperparameters\n",
    "num_epochs = 20  # Number of epochs\n",
    "batch_size = 256  # Batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f23ee449-f22d-4dab-bdd0-15ec73e92c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# 35. Model Training\n",
    "history = model_v1.fit(\n",
    "    X_train,  # Training data\n",
    "    y_train_encoded,  # Encoded training labels\n",
    "    validation_data=(X_val, y_val_encoded),  # Validation data and labels\n",
    "    epochs=num_epochs,  # Number of epochs\n",
    "    batch_size=batch_size,  # Batch size\n",
    "    callbacks=[early_stopping, lr_scheduler]  # Callback functions: early stopping and learning rate scheduler\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc22b476-fb5f-41c8-bd70-83ada47e43c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 36. Extract Training and Validation Loss\n",
    "loss, val_loss = history.history['loss'], history.history['val_loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eaeb77d-b744-4aec-ada7-9f982a20878f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 37. Plot Training and Validation Loss\n",
    "plt.plot(loss, label='Training Loss')  # Plot training loss\n",
    "plt.plot(val_loss, label='Validation Loss')  # Plot validation loss\n",
    "plt.legend()  # Add a legend\n",
    "plt.show()  # Display the plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1af31dd1-d1b0-4470-ae55-21b97eb8ae33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 38. Predictions on Test Data\n",
    "predictions_v1 = model_v1.predict(X_test_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b63130c-d749-48fc-81f5-395d6ff42ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 39. Extract Predicted Labels\n",
    "predictions_v1_labels = predictions_v1.argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "638fa466-866a-42c7-bc17-1b7b033b07bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 40. Print Classification Report\n",
    "print(classification_report(y_test_le, predictions_v1_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81decb39-894a-4c68-81e1-e8f90f4a5b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 41. Print Confusion Matrix\n",
    "print(confusion_matrix(y_test_le, predictions_v1_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8cf197e-61af-440f-b057-9d69e561322e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 42. Print Accuracy Score\n",
    "print(accuracy_score(y_test_le, predictions_v1_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d5c2036-6e01-431e-8ede-57c2181e4aa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 43. Save the Model\n",
    "model_v1.save('model_v1.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78b98a8c-577a-489b-bad0-367dcdd84564",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 44. Load the Model\n",
    "loaded_model = load_model('model_v1.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ca919e7-0e51-46b8-af6a-da2c6c59a315",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 45. New Sentence (sentiment = fear)\n",
    "sentence = \"i even feel a little shaky\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fa33933-ada2-4122-abd8-7c9953de2362",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 46. Create a DataFrame with the Sentence\n",
    "df_new = pd.DataFrame({'Sentence': [sentence]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3506c857-fede-48f6-a328-b213ced78f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 47. Apply the Preprocessing Function\n",
    "df_new['Processed_Sentence'] = df_new['Sentence'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b5673e5-59bd-4c2e-a266-db7123e40305",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 48. Display the DataFrame\n",
    "df_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a190a0eb-c341-4577-9272-46dcf64c3473",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 49. Apply Vectorization\n",
    "df_new_tfidf = tfidf_vectorizer.transform(df_new['Processed_Sentence'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28af7e10-d1b5-4ab1-b5aa-3d2aac8bf380",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 50. Convert to Array\n",
    "df_new_array = df_new_tfidf.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8bb6e3f-8bea-4e72-bb09-58d4f80cc633",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 51. Predictions\n",
    "predictions = loaded_model.predict(df_new_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c356def1-0c15-4546-8e6e-0bd63e1d0700",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 52. Display Predictions\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34cfeeb8-bf6f-40a7-beeb-1d8ea0abb70d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 53. Select the Class with the Highest Probability\n",
    "highest_prob_class = np.argmax(predictions, axis=1)\n",
    "\n",
    "# 54. Display the Class with the Highest Probability\n",
    "highest_prob_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65512679-9d28-4e6c-b3d2-972065da0cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 55. Get the Class Name\n",
    "class_name = label_encoder_v1.inverse_transform(highest_prob_class)\n",
    "\n",
    "# 56. Predicted Class\n",
    "class_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa261539-4f2e-4f08-af1b-d71e89a11d7d",
   "metadata": {},
   "source": [
    "#### Model Implementation: LSTM (Long Short-Term Memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26dd8726-adaf-4700-9aaf-c70ab62b5946",
   "metadata": {},
   "source": [
    "The LSTM is excellent for tasks in Natural Language Processing. It also performs well with time series data, as it works with sequential inputs. These sequences can be text data (sentences) or, for example, time series data.\n",
    "\n",
    "However, LSTM has a significant limitation: it struggles with long-term context. During training, it loses information over the long term due to the vanishing gradient problem. As the gradient diminishes, it eventually disappears, limiting its capacity for larger datasets or extended contexts.\n",
    "\n",
    "If the goal is for the model to learn something extensive, such as the entire content of a book, LSTM wonâ€™t suffice. For that, you need the Transformer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b1a2f1f-264e-4f17-a0f2-d37b0294c82a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 58. Create the Tokenizer\n",
    "tokenizer = Tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33355d43-06a8-4092-8d7a-9a5ad9316b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 59. Fit the Tokenizer with Processed Texts\n",
    "tokenizer.fit_on_texts(training_data['processed_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "702469d7-e4dc-4e09-b1d0-6673ead94f54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 60. Extract Word Index\n",
    "word_index = tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13a692a5-600e-4087-890e-7bb4283dbf0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 61. Check the Length of the Word Index\n",
    "len(word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13bab69b-0be6-433a-a338-b5a818e2c318",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 62. Iterate Over Key-Value Pairs in the Dictionary\n",
    "for i, (key, value) in enumerate(word_index.items()):\n",
    "    print(key, value)\n",
    "    # Break the loop after printing 10 items\n",
    "    if i == 9:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cfcbe16-4511-4a41-aadc-850fa934a1cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 63. Convert Training Texts to Token Sequences\n",
    "train_sequences = tokenizer.texts_to_sequences(training_data['processed_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96725bd1-9b63-4937-9c21-04c6e389426f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 64. Define the Maximum Length of Sequences\n",
    "max_length = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81bd275d-c410-49b0-83dc-6bda81274e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 65. Padding Training Sequences\n",
    "train_sequences_padded = pad_sequences(train_sequences, maxlen=max_length, truncating='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe4b8cb7-3b02-416b-85ec-235ddc62af2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 66. Convert Test Texts to Token Sequences\n",
    "test_sequences = tokenizer.texts_to_sequences(test_data['processed_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4ed7006-5e0b-4f0d-901c-c69f8ab1de3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 67. Padding Test Sequences\n",
    "test_sequences_padded = pad_sequences(test_sequences, maxlen=max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c385b83e-7aca-44bb-aed1-76c836a3b5ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 68. Create the Label Encoder\n",
    "label_encoder_v2 = LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39c56178-2a30-412c-a8fb-d636a18240c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 69. Fit and Transform Sentiment Labels for Training\n",
    "y_train_le = label_encoder_v2.fit_transform(training_data['sentiment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81c33dca-5eb5-46ec-bd52-36d7d13e8844",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 70. Transform Sentiment Labels for Testing\n",
    "y_test_le = label_encoder_v2.transform(test_data['sentiment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0020688b-c938-4fb1-841e-7f4890c4a6b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 71. Convert Labels to Categorical Variables\n",
    "y_train_encoded = to_categorical(y_train_le)\n",
    "y_test_encoded = to_categorical(y_test_le)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d544e0f-ecd2-4c39-9698-1b0a0196ee9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 72. Define Vocabulary Size\n",
    "vocab_size = len(tokenizer.word_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "818d1df8-1bb5-463c-ad79-76e97c487068",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 73. Print Vocabulary Size\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0085653-2262-4cd6-b39b-8c529246c4a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 74. Define Embedding Dimension\n",
    "embedding_dim = max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ef707bc-a975-485e-83db-8abff4d16865",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 75. Construct the LSTM Model\n",
    "model_v2 = tf.keras.Sequential([\n",
    "   \n",
    "    # 75.a Embedding layer with the vocabulary size, embedding dimension, and input length\n",
    "    Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
    "\n",
    "    # 75.b Bidirectional LSTM layer with 64 units\n",
    "    Bidirectional(LSTM(64)),\n",
    "\n",
    "    # 75.c Dropout layer to prevent overfitting\n",
    "    Dropout(0.4),\n",
    "\n",
    "    # 75.d Dense layer with 32 units, Leaky ReLU activation, and L1/L2 regularization\n",
    "    Dense(32, activation='leaky_relu', kernel_regularizer=l1_l2(l1=0.01, l2=0.01)),\n",
    "\n",
    "    # 75.e Additional Dropout layer\n",
    "    Dropout(0.4),\n",
    "\n",
    "    # 75.f Output layer with 6 units and softmax activation for multi-class classification\n",
    "    Dense(6, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "524c1286-f46f-4b0c-afa1-34dad70e542e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 76. Compile the Model\n",
    "model_v2.compile(\n",
    "    loss='categorical_crossentropy',  # Loss function for multi-class classification\n",
    "    optimizer='adam',  # Adam optimizer\n",
    "    metrics=['accuracy', Precision(), Recall(), AUC()]  # Evaluation metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e2c72e8-83d7-41df-905f-085f342edfc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 77. Display Model Summary\n",
    "print(model_v2.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b89302e-2bc1-42f3-bd2e-42f0cc04183f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 78. Define Input Data as Array\n",
    "input_data = np.array(train_sequences_padded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c5202c-7ae7-4d62-90dc-295c29a8de3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 79. Define Output Data as Array\n",
    "output_data = np.array(y_train_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e364866-1bb3-4110-99c0-f666d459ad3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 80. Define Hyperparameters\n",
    "num_epochs = 35  # Number of epochs\n",
    "validation_split_value = 0.2  # Validation split percentage\n",
    "patience = 5  # Early stopping patience"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ed5f5a1-970e-486b-bfe4-d99476f4623a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 81. Configure Early Stopping\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(patience=patience)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98e3c009-c54f-4a11-b1cb-f39be309f7f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 82. Train the Model\n",
    "%%time\n",
    "history = model_v2.fit(\n",
    "    input_data,  # Training input data\n",
    "    output_data,  # Training output data\n",
    "    epochs=num_epochs,  # Number of epochs\n",
    "    verbose=1,  # Verbosity level\n",
    "    validation_split=validation_split_value,  # Validation split ratio\n",
    "    callbacks=[early_stopping]  # Callback for early stopping\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de61b2dd-31e5-4170-a491-948280a85f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 83. Plot Error Curves\n",
    "loss, val_loss = history.history['loss'], history.history['val_loss']  # Extract training and validation loss\n",
    "\n",
    "plt.plot(loss, label='Training Error')  # Plot training error\n",
    "plt.plot(val_loss, label='Validation Error')  # Plot validation error\n",
    "\n",
    "plt.legend()  # Add a legend\n",
    "plt.show()  # Display the plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6516b444-1b02-4994-bdeb-715de7c86f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 84. Predictions on Test Data\n",
    "predictions = model_v2.predict(test_sequences_padded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "007954a1-2c6f-4d89-b6ec-88cdbd660fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 85. Determine Predicted Labels\n",
    "predicted_labels = predictions.argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21164707-2f8f-4ce3-8b27-7e0a7790093d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 86. Display Classification Report\n",
    "print(classification_report(y_test_le, predicted_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e130747f-3aa8-43d0-851c-d3cc57b9a276",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 87. Display Confusion Matrix\n",
    "print(confusion_matrix(y_test_le, predicted_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3442045e-c366-4e87-9ee5-fe7c721c7ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 88. Display Model Accuracy\n",
    "print(accuracy_score(y_test_le, predicted_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2f34f59-8ea5-4c73-a288-a38f8d2b9267",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 89. Save the Model\n",
    "model_v2.save('model_v2.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45878207-e393-4f7c-84e4-581b28e67b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 90. Load the Saved Model\n",
    "loaded_model = load_model('model_v2.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c81d8c4-352f-4688-b2e6-dd28ea6ebe3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 91. New Sentence (Sentiment = Fear)\n",
    "sentence = \"i even feel a little shaky\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "786fa75f-5d0f-4778-a46e-29ba670c9f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 92. Create a DataFrame with the Sentence\n",
    "df_new = pd.DataFrame({'Sentence': [sentence]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abebb706-248c-4b7c-bf9c-495679a03916",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 93. Apply the Preprocessing Function\n",
    "df_new['Processed_Sentence'] = df_new['Sentence'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afd4bb9e-7eda-48db-9cff-ef951dae107f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 94. Process New Data\n",
    "new_sequences = tokenizer.texts_to_sequences(df_new['Processed_Sentence'])\n",
    "new_sequences_padded = pad_sequences(new_sequences, maxlen=max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d0527bb-5c84-4b39-825c-de47739006f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 95. Make Predictions with the Loaded Model\n",
    "predictions = loaded_model.predict(new_sequences_padded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55cf9158-9d81-4fba-804b-f88c6b6866c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 96. Select the Class with the Highest Probability\n",
    "highest_prob_class = np.argmax(predictions, axis=1)\n",
    "\n",
    "# 97. Display the Class with the Highest Probability\n",
    "highest_prob_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "715e6170-61f0-4241-aeea-223342cc2a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 98. Get the Class Name\n",
    "class_name = label_encoder_v2.inverse_transform(highest_prob_class)\n",
    "\n",
    "# 99. Predicted Class\n",
    "class_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "667cd397-1584-44c1-bebd-676fa77e029c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 100. Display Data Processed with SpaCy\n",
    "training_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "547609b6-47b2-4769-843c-ae7d382a26b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 101. Function to Encode Text into Integer Sequences for BERT Input\n",
    "def encode_texts(texts, tokenizer, chunk_size=256, maxlen=512):\n",
    "\n",
    "    # 101.a Configure the tokenizer to truncate texts to the specified maximum length\n",
    "    tokenizer.enable_truncation(max_length=maxlen)\n",
    "\n",
    "    # 101.b Configure the tokenizer to apply padding up to the specified maximum length\n",
    "    tokenizer.enable_padding(length=maxlen)\n",
    "\n",
    "    # 101.c List to store input IDs generated by the tokenizer\n",
    "    input_ids = []\n",
    "\n",
    "    # 101.d List to store attention masks generated by the tokenizer\n",
    "    attention_masks = []\n",
    "\n",
    "    # 101.e Iterate over the texts in chunks of the specified chunk_size\n",
    "    for i in tqdm(range(0, len(texts), chunk_size)):\n",
    "\n",
    "        # 101.f Select a chunk of texts to process\n",
    "        text_chunk = texts[i:i+chunk_size].tolist()\n",
    "\n",
    "        # 101.g Encode the text chunk in batches using the tokenizer\n",
    "        encs = tokenizer.encode_batch(text_chunk)\n",
    "\n",
    "        # 101.h Add the encoded input IDs to the input_ids list\n",
    "        input_ids.extend([enc.ids for enc in encs])\n",
    "\n",
    "        # 101.i Add the generated attention masks to the attention_masks list\n",
    "        attention_masks.extend([enc.attention_mask for enc in encs])\n",
    "\n",
    "    # 101.j Return the input IDs and attention masks as numpy arrays\n",
    "    return np.array(input_ids), np.array(attention_masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e0cc87d-b06b-4d13-a82f-d22896af27e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 102. Load the Pre-Trained Model's Tokenizer\n",
    "bert_tokenizer = transformers.DistilBertTokenizer.from_pretrained(\n",
    "'distilbert-base-multilingual-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afee0a17-8eec-435b-9b47-d2f4b334130c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 103. Save the Tokenizer and Vocabulary Locally\n",
    "bert_tokenizer.save_pretrained('.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "481d93c7-1f7f-46d7-92e2-915c709a1a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 104. Load a Faster Tokenizer Using the Main Tokenizer's Vocabulary\n",
    "from tokenizers import BertWordPieceTokenizer\n",
    "fast_tokenizer = BertWordPieceTokenizer('vocab.txt', lowercase=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70920f99-02a7-4140-9182-572e56b44064",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 105. Visualize the Tokenizer\n",
    "fast_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de77b138-d75b-4dd7-a786-80bb7d802ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 106. Split Data into Training and Validation Sets with Stratified Sampling\n",
    "\n",
    "# Processed text for training\n",
    "X_train, X_valid, Y_train, Y_valid = train_test_split(\n",
    "    training_data['processed_text'].values,  \n",
    "\n",
    "    # Sentiment labels for training\n",
    "    training_data['sentiment'].values,  \n",
    "\n",
    "    # Validation split ratio\n",
    "    test_size=0.2,  \n",
    "\n",
    "    # Random state for reproducibility\n",
    "    random_state=42,  \n",
    "\n",
    "    # Stratified sampling by sentiment labels\n",
    "    stratify=training_data['sentiment']  \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7797ac26-710b-41c3-bceb-0cc186d6beef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 107. Define Maximum Length for Texts\n",
    "max_length = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3128d5f8-803a-4cce-9fec-b81c506375e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 108. Apply Encoding (Tokenization) to the Data\n",
    "X_train_final, train_mask = encode_texts(X_train, fast_tokenizer, maxlen=max_length)\n",
    "X_valid_final, valid_mask = encode_texts(X_valid, fast_tokenizer, maxlen=max_length)\n",
    "X_test_final, test_mask = encode_texts(test_data['processed_text'].to_numpy(), fast_tokenizer, maxlen=max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a698f7b-f9f6-4a2b-be01-68818e248ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 109. Check Shape of Final Training Data\n",
    "X_train_final.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "986ad097-a6ec-4e9d-a4b9-58a57547e540",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 110. Define the Encoder for Output Data\n",
    "label_encoder_v3 = LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a81583dd-6efe-42c8-99a2-4215ec8d695c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 111. Apply the Encoder (fit_transform only on Training Data)\n",
    "y_train_le = label_encoder_v3.fit_transform(Y_train)\n",
    "y_valid_le = label_encoder_v3.transform(Y_valid)\n",
    "y_test_le = label_encoder_v3.transform(test_data['sentiment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aea2982d-7ccf-490d-95ca-4287f73a253a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 112. Convert Output Variable to Categorical\n",
    "y_train_encoded = to_categorical(y_train_le)\n",
    "y_valid_encoded = to_categorical(y_valid_le)\n",
    "y_test_encoded = to_categorical(y_test_le)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b9a92e1-f85a-4f04-a4d6-ae4239d495ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 113. Define Batch Size\n",
    "BATCH_SIZE = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d13c15-214f-4ebf-add2-ab70dba0b309",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 114. Prepare the Training Dataset in the Format Expected by TensorFlow\n",
    "\n",
    "# Combine inputs and labels\n",
    "train_dataset = (\n",
    "    tf.data.Dataset\n",
    "    .from_tensor_slices(((X_train_final, train_mask), y_train_encoded))  \n",
    "\n",
    "    # Repeat the dataset for multiple epochs\n",
    "    .repeat()  \n",
    "\n",
    "    # Shuffle the data with a buffer size of 2048\n",
    "    .shuffle(2048)  \n",
    "\n",
    "    # Group data into batches of the specified size\n",
    "    .batch(BATCH_SIZE)  \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d34030-895d-473f-a8fc-f69a8584f817",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 117. Function to Create the Model\n",
    "def create_model(transformer, max_len=512):\n",
    "\n",
    "    # 117.a Input layer for word IDs\n",
    "    input_word_ids = tf.keras.layers.Input(\n",
    "        shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\"\n",
    "    )\n",
    "\n",
    "    # 117.b Input layer for attention masks\n",
    "    attention_mask = tf.keras.layers.Input(\n",
    "        shape=(max_len,), dtype=tf.int32, name=\"attention_mask\"\n",
    "    )\n",
    "\n",
    "    # 117.c Custom layer for the Transformer\n",
    "    sequence_output = TransformerLayer(transformer)(\n",
    "        [input_word_ids, attention_mask]\n",
    "    )\n",
    "\n",
    "    # 117.d Select the CLS token (first token)\n",
    "    cls_token = sequence_output[:, 0, :]\n",
    "\n",
    "    # 117.e Dense layer with softmax activation for classification\n",
    "    out = Dense(6, activation=\"softmax\")(cls_token)\n",
    "\n",
    "    # 117.f Keras model definition\n",
    "    model = tf.keras.Model(\n",
    "        inputs=[input_word_ids, attention_mask], outputs=out\n",
    "    )\n",
    "\n",
    "    # 117.g Compile the model\n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=1e-5),\n",
    "        loss=\"categorical_crossentropy\",\n",
    "        metrics=[\"accuracy\", Precision(), Recall(), AUC()],\n",
    "    )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b009de9-bd95-4876-a045-550e802bc659",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 119. Load the Pre-Trained Model\n",
    "transformer_layer = TFDistilBertModel.from_pretrained(\n",
    "    \"distilbert-base-multilingual-cased\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1739e3fa-b44f-45ba-9bcc-c50fec6be79b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 120. Create the Model with the Pre-Trained Transformer Layers and Custom Layers for Fine-Tuning\n",
    "model_v3 = create_model(transformer_layer, max_len=max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8443f3ae-f918-4800-b3ac-96687f71da0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 121. Display Model Summary\n",
    "model_v3.summary()\n",
    "\n",
    "# 122. Set the First Three Layers of the Model as Non-Trainable\n",
    "model_v3.layers[0].trainable = False\n",
    "model_v3.layers[1].trainable = False\n",
    "model_v3.layers[2].trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8429c178-2731-4d38-8056-3f0fea290101",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 123. Display Updated Model Summary\n",
    "model_v3.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce28845a-9408-4728-a26c-bc1bd9a61ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 124. Define Hyperparameters\n",
    "n_steps = X_train_final.shape[0] // BATCH_SIZE  # Number of steps per epoch\n",
    "num_epochs = 3  # Number of epochs\n",
    "\n",
    "# 125. Train the Model\n",
    "%%time\n",
    "history = model_v3.fit(\n",
    "    train_dataset,  # Training dataset\n",
    "    steps_per_epoch=n_steps,  # Number of steps per epoch\n",
    "    validation_data=valid_dataset,  # Validation dataset\n",
    "    epochs=num_epochs  # Number of epochs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e29c6ad-4238-4c05-9408-4e6e12ae2352",
   "metadata": {},
   "source": [
    "#### Model Implementation: Custom Tokenization Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61d2475b-e1b6-4ae1-bfc7-fd728dcf3829",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 126. Plot Learning Curves\n",
    "loss, val_loss = history.history['loss'], history.history['val_loss']  # Extract training and validation loss\n",
    "\n",
    "plt.plot(loss, label='Training Error')  # Plot training error\n",
    "plt.plot(val_loss, label='Validation Error')  # Plot validation error\n",
    "\n",
    "plt.legend()  # Add a legend\n",
    "plt.show()  # Display the plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9679490f-514d-4f50-b5a8-7e10622d4dc5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b8bd1a7-6083-4832-bbd7-5f4276923201",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d02c5f52-1762-4325-81ec-bdc7799cb1b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 129. Extract Predicted Labels\n",
    "predicted_labels = predictions.argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5021c6cb-5f92-44d3-9bec-612db68107bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 130. Print Classification Report\n",
    "print(classification_report(y_test_le, predicted_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3832a5b-ac84-4315-81f0-450b6f666184",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 131. Print Confusion Matrix\n",
    "print(confusion_matrix(y_test_le, predicted_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "546a92c3-9a1d-44d7-8321-b1426b82bacd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 132. Print Accuracy Score\n",
    "print(accuracy_score(y_test_le, predicted_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30d6858e-6ffb-4292-9c33-b9bb92dd67b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 133. Save the Model in TensorFlow Format\n",
    "model_v3.save(\"model_v3.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9898161c-05a0-4e2d-9b5c-85f26f66e48a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 134. Load the Model\n",
    "\n",
    "# 134.a Imports\n",
    "from transformers import TFDistilBertModel\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.utils import custom_object_scope\n",
    "\n",
    "# 134.b Load the model with the custom layer registered\n",
    "reloaded_model = load_model(\n",
    "    \"model_v3.keras\",\n",
    "    custom_objects={\"TransformerLayer\": TransformerLayer}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "890b2704-ea41-44e1-b8b6-ef4ff7cd38fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 135. New Sentence (Sentiment = Fear)\n",
    "sentence = \"i even feel a little shaky\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21d49828-1322-47da-b835-9ec4364313eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 136. Create a DataFrame with the Sentence\n",
    "df_new = pd.DataFrame({'Sentence': [sentence]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b1d9d0b-aef7-48d2-af49-1555c1e34cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 137. Apply the Preprocessing Function\n",
    "df_new['Processed_Sentence'] = df_new['Sentence'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ae4ee7e-9e0b-4977-a355-5ffb67e219f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 138. Encode the New Data\n",
    "new_data = encode_texts(\n",
    "df_new['Processed_Sentence'], fast_tokenizer, maxlen=max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9c05dc5-14bc-4c13-82a2-86764aea8acd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 139. Make Predictions with the Loaded Model\n",
    "predictions = model_v3.predict(new_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46455e00-c7d2-448c-8ed3-cd7c8e40147e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 140. Select the Class with the Highest Probability\n",
    "highest_prob_class = np.argmax(predictions, axis=1)\n",
    "\n",
    "# 141. Display the Class with the Highest Probability\n",
    "highest_prob_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e036a03-effe-45fb-baa3-77ffe1addc72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 142. Get the Class Name\n",
    "class_name = label_encoder_v3.inverse_transform(highest_prob_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb3d5842-1aec-4b45-b349-e0107551b3e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 143. Predicted Class\n",
    "class_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8833f345-2b8e-4ec9-9de4-9eda6255af85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 144. Disable Parallelism in the Transformers Package\n",
    "%env TOKENIZERS_PARALLELISM=false"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

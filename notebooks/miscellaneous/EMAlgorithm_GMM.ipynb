{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7f777ba4-dbe8-40dc-b8a5-a8730b8dbfb8",
   "metadata": {},
   "source": [
    "## [EM Algorithm and Gaussian Mixture Models for Advanced Data Clustering](https://medium.com/data-science-collective/the-em-algorithm-and-gaussian-mixture-models-for-advanced-data-clustering-948756fe76c9)\n",
    "\n",
    "> A deep dive into the core concepts of unsupervised clustering with practical application on customer data segmentation\n",
    "\n",
    "\n",
    "The **Expectation-Maximization (EM)** algorithm, particularly its application to **Gaussian Mixture Models (GMM)**, is a foundational unsupervised learning technique."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c5f7915-fee6-4294-a796-ec51e214880b",
   "metadata": {},
   "source": [
    "A **_Gaussian Mixture Model (GMM)_** is a probabilistic model that represents the data as a combination of multiple Gaussian distributions.\n",
    "\n",
    "It's a clustering tool for unsupervised learning, offering more flexibility than other clustering methods like k-means by computing probabilities of data points to each cluster.\n",
    "\n",
    "Its key features include:\n",
    "\n",
    "- **Probabilistic Model**: Generates the likelihood of different outcomes using probability distribution, rather than predicting a single, definite result.\n",
    "- **Unsupervised Learning**: No labeled data required for training.\n",
    "- **Clustering and Density Estimation**: Clusters data points into different groups and estimates the probability density function (PDF) of the data.\n",
    "- **Flexibility in Modeling**: Can approximate any distribution represented as a weighted sum of normal distributions.\n",
    "\n",
    "GMMs are commonly used in various fields like financial investments, natural language analysis, predictive maintenance, and medical imaging (MRI, CT scans)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08f28226-025a-4a30-a7cb-3d019cd6dfe1",
   "metadata": {},
   "source": [
    "The core concept of GMMs is to assume that the data points are generated from a mixture of multiple Gaussian distributions, each of which has its own model parameters:\n",
    "\n",
    "- _mean_ (`μ_k`),\n",
    "- _variance_ (or _covariance_) (`Σ_k`), and\n",
    "- _mixing coefficient_ (`ϕ_k`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e78caccc-f82f-4ccf-8d71-8c937c64a208",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q numpy pandas matplotlib scikit-learn ucimlrepo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "97c2cb26-372a-4c66-ba90-3acb66e663e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "142a05ec-cc99-4d4f-af56-a42cb5f02bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from ucimlrepo import fetch_ucirepo\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "67eede0d-ca33-4b59-a7d0-2eaad7f7b358",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------Metadata Information------\n",
      "\n",
      "{'uci_id': 563,\n",
      " 'name': 'Iranian Churn',\n",
      " 'repository_url': 'https://archive.ics.uci.edu/dataset/563/iranian+churn+dataset',\n",
      " 'data_url': 'https://archive.ics.uci.edu/static/public/563/data.csv',\n",
      " 'abstract': 'This dataset is randomly collected from an Iranian telecom '\n",
      "             \"company's database over a period of 12 months.\",\n",
      " 'area': 'Business',\n",
      " 'tasks': ['Classification', 'Regression'],\n",
      " 'characteristics': ['Multivariate'],\n",
      " 'num_instances': 3150,\n",
      " 'num_features': 13,\n",
      " 'feature_types': ['Integer'],\n",
      " 'demographics': ['Age'],\n",
      " 'target_col': ['Churn'],\n",
      " 'index_col': None,\n",
      " 'has_missing_values': 'no',\n",
      " 'missing_values_symbol': None,\n",
      " 'year_of_dataset_creation': 2020,\n",
      " 'last_updated': 'Sat Mar 09 2024',\n",
      " 'dataset_doi': '10.24432/C5JW3Z',\n",
      " 'creators': [],\n",
      " 'intro_paper': None,\n",
      " 'additional_info': {'summary': 'This dataset is randomly collected from an '\n",
      "                                'Iranian telecom companyâ€™s database over a '\n",
      "                                'period of 12 months. A total of 3150 rows of '\n",
      "                                'data, each representing a customer, bear '\n",
      "                                'information for 13 columns. The attributes '\n",
      "                                'that are in this dataset\\r\\n'\n",
      "                                'are call failures, frequency of SMS, number '\n",
      "                                'of complaints, number of distinct calls, '\n",
      "                                'subscription length, age group, the charge '\n",
      "                                'amount, type of service, seconds of use, '\n",
      "                                'status, frequency of use, and Customer '\n",
      "                                'Value.\\r\\n'\n",
      "                                '\\r\\n'\n",
      "                                'All of the attributes except for attribute '\n",
      "                                'churn is the aggregated data of the first 9 '\n",
      "                                'months. The churn labels are the state of the '\n",
      "                                'customers at the end of 12 months. The three '\n",
      "                                'months is the designated planning gap.',\n",
      "                     'purpose': None,\n",
      "                     'funded_by': None,\n",
      "                     'instances_represent': None,\n",
      "                     'recommended_data_splits': None,\n",
      "                     'sensitive_data': None,\n",
      "                     'preprocessing_description': None,\n",
      "                     'variable_info': 'Anonymous Customer ID\\r\\n'\n",
      "                                      'Call Failures: number of call '\n",
      "                                      'failures\\r\\n'\n",
      "                                      'Complains: binary (0: No complaint, 1: '\n",
      "                                      'complaint)\\r\\n'\n",
      "                                      'Subscription Length: total months of '\n",
      "                                      'subscription\\r\\n'\n",
      "                                      'Charge Amount: Ordinal attribute (0: '\n",
      "                                      'lowest amount, 9: highest amount)\\r\\n'\n",
      "                                      'Seconds of Use: total seconds of '\n",
      "                                      'calls\\r\\n'\n",
      "                                      'Frequency of use: total number of '\n",
      "                                      'calls\\r\\n'\n",
      "                                      'Frequency of SMS: total number of text '\n",
      "                                      'messages\\r\\n'\n",
      "                                      'Distinct Called Numbers: total number '\n",
      "                                      'of distinct phone calls \\r\\n'\n",
      "                                      'Age Group: ordinal attribute (1: '\n",
      "                                      'younger age, 5: older age)\\r\\n'\n",
      "                                      'Tariff Plan: binary (1: Pay as you go, '\n",
      "                                      '2: contractual)\\r\\n'\n",
      "                                      'Status: binary (1: active, 2: '\n",
      "                                      'non-active)\\r\\n'\n",
      "                                      'Churn: binary (1: churn, 0: non-churn) '\n",
      "                                      '- Class label\\r\\n'\n",
      "                                      'Customer Value: The calculated value of '\n",
      "                                      'customer',\n",
      "                     'citation': None}}\n",
      "------Variable Information\n",
      "                        name     role        type demographic description  \\\n",
      "0             Call  Failure  Feature     Integer        None        None   \n",
      "1                 Complains  Feature      Binary        None        None   \n",
      "2      Subscription  Length  Feature     Integer        None        None   \n",
      "3            Charge  Amount  Feature     Integer        None        None   \n",
      "4            Seconds of Use  Feature     Integer        None        None   \n",
      "5          Frequency of use  Feature     Integer        None        None   \n",
      "6          Frequency of SMS  Feature     Integer        None        None   \n",
      "7   Distinct Called Numbers  Feature     Integer        None        None   \n",
      "8                 Age Group  Feature     Integer         Age        None   \n",
      "9               Tariff Plan  Feature     Integer        None        None   \n",
      "10                   Status  Feature      Binary        None        None   \n",
      "11                      Age  Feature     Integer         Age        None   \n",
      "12           Customer Value  Feature  Continuous        None        None   \n",
      "13                    Churn   Target      Binary        None        None   \n",
      "\n",
      "   units missing_values  \n",
      "0   None             no  \n",
      "1   None             no  \n",
      "2   None             no  \n",
      "3   None             no  \n",
      "4   None             no  \n",
      "5   None             no  \n",
      "6   None             no  \n",
      "7   None             no  \n",
      "8   None             no  \n",
      "9   None             no  \n",
      "10  None             no  \n",
      "11  None             no  \n",
      "12  None             no  \n",
      "13  None             no  \n"
     ]
    }
   ],
   "source": [
    "# fetch dataset\n",
    "iranian_churn = fetch_ucirepo(id=563)\n",
    "\n",
    "# data (as pandas dataframes)\n",
    "features = iranian_churn.data.features\n",
    "targets = iranian_churn.data.targets\n",
    "\n",
    "# metadata\n",
    "print(\"------Metadata Information------\\n\")\n",
    "pprint.pp(iranian_churn.metadata)\n",
    "  \n",
    "# variable information\n",
    "print(\"------Variable Information\\n\", iranian_churn.variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0a6c0421-69fc-4bc7-b420-04cd8cb8960b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(iranian_churn.metadata.data_url)\n",
    "\n",
    "df.rename(columns=\n",
    "          {'Subscription  Length': 'subscription_length',\n",
    "           'Call  Failure': 'call_failure',\n",
    "           'Complains': 'complains',\n",
    "           'Charge  Amount': 'charge_amount',\n",
    "           'Frequency of use': 'frequency_of_use',\n",
    "           'Seconds of Use': 'seconds_of_use',\n",
    "           'Frequency of SMS': 'frequency_of_sms',\n",
    "           'Distinct Called Numbers': 'distinct_called_numbers',\n",
    "           'Age Group': 'age_group',\n",
    "           'Tariff Plan': 'tariff_plan',\n",
    "           'Status': 'status',\n",
    "           'Age': 'age',\n",
    "           'Customer Value': 'customer_value',\n",
    "           'Churn': 'churn'}\n",
    ", inplace=True)\n",
    "\n",
    "cols = ['subscription_length', 'customer_value', 'age', 'frequency_of_use', 'churn']\n",
    "X = df[cols]\n",
    "\n",
    "num_features = ['subscription_length', 'customer_value', 'age', 'frequency_of_use']\n",
    "num_transformer = Pipeline(steps=[('scaler', StandardScaler())])\n",
    "\n",
    "cat_features = ['churn']\n",
    "cat_transformer = Pipeline(steps=[('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', num_transformer, num_features),\n",
    "        ('cat', cat_transformer, cat_features)\n",
    "    ],\n",
    "    remainder='passthrough'\n",
    ")\n",
    "\n",
    "X_processed = preprocessor.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9983dbb8-bb2a-4f70-b10f-31b88cbbf8a4",
   "metadata": {},
   "source": [
    "#### Model Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "60f6e50d-1378-4437-9261-f71d6cea59f1",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 18\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# gmm\u001b[39;00m\n\u001b[32m      5\u001b[39m gmm =  GaussianMixture(\n\u001b[32m      6\u001b[39m     n_components=\u001b[32m10\u001b[39m,            \u001b[38;5;66;03m# cluseter 10 Gaussian components\u001b[39;00m\n\u001b[32m      7\u001b[39m     covariance_type=\u001b[33m'\u001b[39m\u001b[33mfull\u001b[39m\u001b[33m'\u001b[39m,     \u001b[38;5;66;03m# 'full' for complex underlying data distribution (not spherical)\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     16\u001b[39m     random_state=\u001b[32m42\u001b[39m,\n\u001b[32m     17\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m gmm_labels = \u001b[43mgmm\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit_predict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_processed\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     21\u001b[39m \u001b[38;5;66;03m# k-means (for comparison)\u001b[39;00m\n\u001b[32m     22\u001b[39m kmeans = KMeans(\n\u001b[32m     23\u001b[39m     n_clusters=\u001b[32m5\u001b[39m, \n\u001b[32m     24\u001b[39m     init=\u001b[33m\"\u001b[39m\u001b[33mk-means++\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     29\u001b[39m     algorithm=\u001b[33m'\u001b[39m\u001b[33mlloyd\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m     30\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\WORKSPACE\\GitHub\\shortthirdman\\Jupyter Notebooks\\PredictorXVault-Cookbook\\dev\\Lib\\site-packages\\sklearn\\base.py:1363\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1356\u001b[39m     estimator._validate_params()\n\u001b[32m   1358\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1359\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1360\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1361\u001b[39m     )\n\u001b[32m   1362\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1363\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\WORKSPACE\\GitHub\\shortthirdman\\Jupyter Notebooks\\PredictorXVault-Cookbook\\dev\\Lib\\site-packages\\sklearn\\mixture\\_base.py:250\u001b[39m, in \u001b[36mBaseMixture.fit_predict\u001b[39m\u001b[34m(self, X, y)\u001b[39m\n\u001b[32m    247\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m n_iter \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m1\u001b[39m, \u001b[38;5;28mself\u001b[39m.max_iter + \u001b[32m1\u001b[39m):\n\u001b[32m    248\u001b[39m     prev_lower_bound = lower_bound\n\u001b[32m--> \u001b[39m\u001b[32m250\u001b[39m     log_prob_norm, log_resp = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_e_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    251\u001b[39m     \u001b[38;5;28mself\u001b[39m._m_step(X, log_resp)\n\u001b[32m    252\u001b[39m     lower_bound = \u001b[38;5;28mself\u001b[39m._compute_lower_bound(log_resp, log_prob_norm)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\WORKSPACE\\GitHub\\shortthirdman\\Jupyter Notebooks\\PredictorXVault-Cookbook\\dev\\Lib\\site-packages\\sklearn\\mixture\\_base.py:312\u001b[39m, in \u001b[36mBaseMixture._e_step\u001b[39m\u001b[34m(self, X)\u001b[39m\n\u001b[32m    296\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_e_step\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[32m    297\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"E step.\u001b[39;00m\n\u001b[32m    298\u001b[39m \n\u001b[32m    299\u001b[39m \u001b[33;03m    Parameters\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    310\u001b[39m \u001b[33;03m        the point of each sample in X.\u001b[39;00m\n\u001b[32m    311\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m312\u001b[39m     log_prob_norm, log_resp = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_estimate_log_prob_resp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    313\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m np.mean(log_prob_norm), log_resp\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\WORKSPACE\\GitHub\\shortthirdman\\Jupyter Notebooks\\PredictorXVault-Cookbook\\dev\\Lib\\site-packages\\sklearn\\mixture\\_base.py:532\u001b[39m, in \u001b[36mBaseMixture._estimate_log_prob_resp\u001b[39m\u001b[34m(self, X)\u001b[39m\n\u001b[32m    513\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_estimate_log_prob_resp\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[32m    514\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Estimate log probabilities and responsibilities for each sample.\u001b[39;00m\n\u001b[32m    515\u001b[39m \n\u001b[32m    516\u001b[39m \u001b[33;03m    Compute the log probabilities, weighted log probabilities per\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    530\u001b[39m \u001b[33;03m        logarithm of the responsibilities\u001b[39;00m\n\u001b[32m    531\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m532\u001b[39m     weighted_log_prob = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_estimate_weighted_log_prob\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    533\u001b[39m     log_prob_norm = logsumexp(weighted_log_prob, axis=\u001b[32m1\u001b[39m)\n\u001b[32m    534\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m np.errstate(under=\u001b[33m\"\u001b[39m\u001b[33mignore\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    535\u001b[39m         \u001b[38;5;66;03m# ignore underflow\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\WORKSPACE\\GitHub\\shortthirdman\\Jupyter Notebooks\\PredictorXVault-Cookbook\\dev\\Lib\\site-packages\\sklearn\\mixture\\_base.py:485\u001b[39m, in \u001b[36mBaseMixture._estimate_weighted_log_prob\u001b[39m\u001b[34m(self, X)\u001b[39m\n\u001b[32m    474\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_estimate_weighted_log_prob\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[32m    475\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Estimate the weighted log-probabilities, log P(X | Z) + log weights.\u001b[39;00m\n\u001b[32m    476\u001b[39m \n\u001b[32m    477\u001b[39m \u001b[33;03m    Parameters\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    483\u001b[39m \u001b[33;03m    weighted_log_prob : array, shape (n_samples, n_component)\u001b[39;00m\n\u001b[32m    484\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m485\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_estimate_log_prob\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m + \u001b[38;5;28mself\u001b[39m._estimate_log_weights()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\WORKSPACE\\GitHub\\shortthirdman\\Jupyter Notebooks\\PredictorXVault-Cookbook\\dev\\Lib\\site-packages\\sklearn\\mixture\\_gaussian_mixture.py:839\u001b[39m, in \u001b[36mGaussianMixture._estimate_log_prob\u001b[39m\u001b[34m(self, X)\u001b[39m\n\u001b[32m    838\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_estimate_log_prob\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[32m--> \u001b[39m\u001b[32m839\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_estimate_log_gaussian_prob\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    840\u001b[39m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmeans_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mprecisions_cholesky_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcovariance_type\u001b[49m\n\u001b[32m    841\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\WORKSPACE\\GitHub\\shortthirdman\\Jupyter Notebooks\\PredictorXVault-Cookbook\\dev\\Lib\\site-packages\\sklearn\\mixture\\_gaussian_mixture.py:487\u001b[39m, in \u001b[36m_estimate_log_gaussian_prob\u001b[39m\u001b[34m(X, means, precisions_chol, covariance_type)\u001b[39m\n\u001b[32m    485\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m k, (mu, prec_chol) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mzip\u001b[39m(means, precisions_chol)):\n\u001b[32m    486\u001b[39m         y = np.dot(X, prec_chol) - np.dot(mu, prec_chol)\n\u001b[32m--> \u001b[39m\u001b[32m487\u001b[39m         log_prob[:, k] = \u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43msum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43msquare\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    489\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m covariance_type == \u001b[33m\"\u001b[39m\u001b[33mtied\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    490\u001b[39m     log_prob = np.empty((n_samples, n_components), dtype=X.dtype)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\WORKSPACE\\GitHub\\shortthirdman\\Jupyter Notebooks\\PredictorXVault-Cookbook\\dev\\Lib\\site-packages\\numpy\\_core\\fromnumeric.py:2466\u001b[39m, in \u001b[36msum\u001b[39m\u001b[34m(a, axis, dtype, out, keepdims, initial, where)\u001b[39m\n\u001b[32m   2463\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m out\n\u001b[32m   2464\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m res\n\u001b[32m-> \u001b[39m\u001b[32m2466\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_wrapreduction\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2467\u001b[39m \u001b[43m    \u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43madd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43msum\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2468\u001b[39m \u001b[43m    \u001b[49m\u001b[43mkeepdims\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkeepdims\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minitial\u001b[49m\u001b[43m=\u001b[49m\u001b[43minitial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwhere\u001b[49m\u001b[43m=\u001b[49m\u001b[43mwhere\u001b[49m\n\u001b[32m   2469\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\WORKSPACE\\GitHub\\shortthirdman\\Jupyter Notebooks\\PredictorXVault-Cookbook\\dev\\Lib\\site-packages\\numpy\\_core\\fromnumeric.py:86\u001b[39m, in \u001b[36m_wrapreduction\u001b[39m\u001b[34m(obj, ufunc, method, axis, dtype, out, **kwargs)\u001b[39m\n\u001b[32m     83\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     84\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m reduction(axis=axis, out=out, **passkwargs)\n\u001b[32m---> \u001b[39m\u001b[32m86\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mufunc\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreduce\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mpasskwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# gmm\n",
    "gmm =  GaussianMixture(\n",
    "    n_components=10,            # cluseter 10 Gaussian components\n",
    "    covariance_type='full',     # 'full' for complex underlying data distribution (not spherical)\n",
    "    tol=1e-10,                  # stop the iteration when increase in log-likelihood hits the tol\n",
    "    reg_covar=1e-10,            # add to the covariance to ensure positive definite matrices\n",
    "    max_iter=1000,              # 1,000 epochs\n",
    "    n_init=1000,                # 1,000 init model runs with diff initializations of the parameters\n",
    "    init_params='kmeans',       # default\n",
    "    weights_init=None,          # init model params to set to None (without clues)\n",
    "    means_init=None,\n",
    "    precisions_init=None,\n",
    "    random_state=42,\n",
    ")\n",
    "gmm_labels = gmm.fit_predict(X_processed)\n",
    "\n",
    "\n",
    "# k-means (for comparison)\n",
    "kmeans = KMeans(\n",
    "    n_clusters=5, \n",
    "    init=\"k-means++\",\n",
    "    n_init=500,\n",
    "    max_iter=500,\n",
    "    tol=1e-10,\n",
    "    random_state=42,\n",
    "    algorithm='lloyd'\n",
    ")\n",
    "kmeans_labels = kmeans.fit_predict(X_processed)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

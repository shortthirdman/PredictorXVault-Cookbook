{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### [Gradient Boosting Decision Tree (GBDT) Regression Model](https://medium.com/top-python-libraries/python-implementation-of-gradient-boosting-decision-tree-gbdt-regression-model-with-residual-04d0174a46a5)"
      ],
      "metadata": {
        "id": "6YyOiwDEwg_N"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SnkSWAwuwa1_",
        "outputId": "3c47a4a2-c1f1-4f36-ff33-38f41f9d7fc0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/308.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m307.2/308.4 kB\u001b[0m \u001b[31m50.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m308.4/308.4 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q -U joblib shap PyALE"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import time\n",
        "import shap\n",
        "import joblib\n",
        "import warnings\n",
        "import matplotlib\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from PyALE import ale\n",
        "\n",
        "from itertools import combinations\n",
        "from collections import defaultdict\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.metrics import mean_absolute_error, r2_score, mean_squared_error\n",
        "from sklearn.inspection import PartialDependenceDisplay, partial_dependence\n",
        "\n",
        "from scipy.stats import gaussian_kde"
      ],
      "metadata": {
        "id": "8tJzw8TWwx9p"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Global Settings ---\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"sklearn.utils._bunch\")\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning)  # Ignore known data type mismatch warnings\n",
        "matplotlib.use('TkAgg')  # It is recommended to set this at the top of the script to avoid backend conflicts\n",
        "\n",
        "print('-------------------------------------Preparing Data---------------------------------------')\n",
        "# Please ensure the file path is correct\n",
        "df = pd.read_excel(r'D:\\data.xlsx')\n",
        "\n",
        "# --- Data Processing Correction: Use Pandas format throughout to retain feature names ---\n",
        "y = df.iloc[:, 0]\n",
        "x = df.iloc[:, 1:]\n",
        "feature_names_from_df = x.columns.tolist()"
      ],
      "metadata": {
        "id": "kxdVu6P81MYE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=42)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled_df = pd.DataFrame(scaler.fit_transform(x_train), columns=x_train.columns, index=x_train.index)\n",
        "X_test_scaled_df = pd.DataFrame(scaler.transform(x_test), columns=x_test.columns, index=x_test.index)"
      ],
      "metadata": {
        "id": "9uTgrSkw1SAZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('-------------------------------------Defining GBDT model hyperparameter range---------------------------------------')\n",
        "\n",
        "n_estimators_range = range(100, 301, 100)\n",
        "max_depth_range = range(3, 7, 1)\n",
        "learning_rate_range = [0.01, 0.1, 0.2]\n",
        "\n",
        "print('-------------------------------------Searching for the best hyperparameters---------------------------------------')\n",
        "\n",
        "gbdt_param_grid = {\n",
        "    'n_estimators': n_estimators_range,\n",
        "    'max_depth': max_depth_range,\n",
        "    'learning_rate': learning_rate_range\n",
        "}\n",
        "\n",
        "gd = GridSearchCV(estimator=GradientBoostingRegressor(random_state=0),\n",
        "                  param_grid=gbdt_param_grid,\n",
        "                  cv=5,\n",
        "                  n_jobs=-1,\n",
        "                  verbose=0)\n",
        "\n",
        "gd.fit(X_train_scaled_df, y_train)\n",
        "\n",
        "print('-------------------------------------Outputting the best model---------------------------------------')\n",
        "print(\"Best result validated in cross-validation:\", gd.best_score_)\n",
        "print(\"Best parameter model:\", gd.best_estimator_)\n",
        "print(\"(dict) Best parameters:\", gd.best_params_)\n",
        "\n",
        "print('-------------------------------------Saving the best model---------------------------------------')\n",
        "\n",
        "model_save_dir = r'D:\\folder'\n",
        "\n",
        "os.makedirs(model_save_dir, exist_ok=True)\n",
        "model_path = os.path.join(model_save_dir, 'GBDT_model_final.joblib')\n",
        "joblib.dump(gd.best_estimator_, model_path)\n",
        "print(f\"Model has been saved to: {model_path}\")\n",
        "loaded_model = joblib.load(model_path)"
      ],
      "metadata": {
        "id": "kD3jLkJt1Vyw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('-------------------------------Apply the model--------------------------------------')\n",
        "y_test_pred = loaded_model.predict(X_test_scaled_df)\n",
        "y_train_pred = loaded_model.predict(X_train_scaled_df)\n",
        "\n",
        "print('-------------------------------------Train the model---------------------------------------')\n",
        "train_mse = mean_squared_error(y_train, y_train_pred)\n",
        "train_rmse = np.sqrt(train_mse)\n",
        "train_mae = mean_absolute_error(y_train, y_train_pred)\n",
        "train_r2 = r2_score(y_train, y_train_pred)\n",
        "train_n = len(X_train_scaled_df)\n",
        "print(f'MSE: {train_mse:.4f}, RMSE: {train_rmse:.4f}, MAE: {train_mae:.4f}, R2: {train_r2:.4f}, N: {train_n}')\n",
        "\n",
        "print('-------------------------------------Evaluate the model performance---------------------------------------')\n",
        "test_mse = mean_squared_error(y_test, y_test_pred)\n",
        "test_rmse = np.sqrt(test_mse)\n",
        "test_mae = mean_absolute_error(y_test, y_test_pred)\n",
        "test_r2 = r2_score(y_test, y_test_pred)\n",
        "test_n = len(X_test_scaled_df)\n",
        "print(f'MSE: {test_mse:.4f}, RMSE: {test_rmse:.4f}, MAE: {test_mae:.4f}, R2: {test_r2:.4f}, N: {test_n}')\n",
        "\n",
        "print('----------------------------------------Plot the reuslt-----------------------------------------')\n",
        "results_plot_save_dir = r'D:\\foler'\n",
        "\n",
        "\n",
        "def plot_regression_fit(y_true, y_pred, r2, rmse, mae, data_label_en, title_en, save_path):\n",
        "    plt.style.use('seaborn-v0_8-whitegrid')\n",
        "    plt.rcParams['font.family'] = 'sans-serif'\n",
        "    fig, ax = plt.subplots(figsize=(7, 7))\n",
        "    ax.scatter(y_true, y_pred, alpha=0.6, edgecolors='k', label=f'{data_label_en} (n={len(y_true)})')\n",
        "    lims = [np.min([y_true.min(), y_pred.min()]) - 5, np.max([y_true.max(), y_pred.max()]) + 5]\n",
        "    ax.plot(lims, lims, 'k--', alpha=0.75, zorder=0, label='1:1 Line (Perfect Fit)')\n",
        "    ax.set_aspect('equal')\n",
        "    ax.set_xlim(lims)\n",
        "    ax.set_ylim(lims)\n",
        "    # Ensure y_true and y_pred are numpy arrays for polynomial fitting\n",
        "    y_true_np = np.array(y_true)\n",
        "    y_pred_np = np.array(y_pred)\n",
        "    m, b = np.polyfit(y_true_np, y_pred_np, 1)\n",
        "    ax.plot(y_true_np, m * y_true_np + b, 'r-', label='Linear Fit')\n",
        "    ax.set_xlabel('True Values (%)', fontsize=12)\n",
        "    ax.set_ylabel('Predicted Values (%)', fontsize=12)\n",
        "    ax.set_title(title_en, fontsize=14, weight='bold')\n",
        "    metrics_text = f'$R^2 = {r2:.3f}$\\n$RMSE = {rmse:.3f}$\\n$MAE = {mae:.3f}$'\n",
        "    ax.text(0.05, 0.95, metrics_text, transform=ax.transAxes, fontsize=12, verticalalignment='top',\n",
        "            bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
        "    ax.legend(loc='lower right')\n",
        "    fig.savefig(save_path, dpi=200, bbox_inches='tight')\n",
        "    plt.close(fig)\n",
        "\n",
        "\n",
        "train_path = os.path.join(results_plot_save_dir, 'GBDT_TrainingSet_final.png')\n",
        "test_path = os.path.join(results_plot_save_dir, 'GBDT_TestSet_final.png')\n",
        "plot_regression_fit(y_train, y_train_pred, train_r2, train_rmse, train_mae, 'Train Set',\n",
        "                    'GBDT Model Performance (Train Set)', train_path)\n",
        "plot_regression_fit(y_test, y_test_pred, test_r2, test_rmse, test_mae, 'Test Set', 'GBDT Model Performance (Test Set)',\n",
        "                    test_path)\n",
        "\n",
        "plt.rcdefaults()"
      ],
      "metadata": {
        "id": "z7CIpz6b1ZYp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_importance_combined(df_importance, title, save_path, bar_color='dodgerblue'):\n",
        "    \"\"\"\n",
        "    A general-purpose function for plotting a combination chart of a\n",
        "    'horizontal bar plot + inset donut chart'.\n",
        "\n",
        "    Parameters:\n",
        "    - df_importance: DataFrame containing 'Feature' and 'Importance' columns.\n",
        "    - title: The main title of the chart.\n",
        "    - save_path: The file path to save the image.\n",
        "    - bar_color: The color of the bars in the bar chart.\n",
        "    \"\"\"\n",
        "    # To plot a horizontal bar chart with the most important feature at the top,\n",
        "    # sort the DataFrame by importance in ascending order.\n",
        "    df_importance_sorted = df_importance.sort_values(by='Importance', ascending=True)\n",
        "\n",
        "\n",
        "    plt.rc(\"font\", family='sans-serif')\n",
        "\n",
        "    # Create the main figure and axes.\n",
        "    fig, ax = plt.subplots(figsize=(14, 10))\n",
        "\n",
        "    # Plot the horizontal bar chart.\n",
        "    bars = ax.barh(df_importance_sorted['Feature'], df_importance_sorted['Importance'], color=bar_color, alpha=0.8)\n",
        "    ax.set_title(title, fontsize=18, pad=20)\n",
        "    ax.set_xlabel('Importance Score', fontsize=14)\n",
        "    ax.set_ylabel('Feature', fontsize=14)\n",
        "    ax.tick_params(axis='both', which='major', labelsize=12)\n",
        "    ax.grid(axis='x', linestyle='--', alpha=0.6)\n",
        "\n",
        "    # Add value labels to the bars.\n",
        "    for bar in bars:\n",
        "        width = bar.get_width()\n",
        "        ax.text(width, bar.get_y() + bar.get_height() / 2,\n",
        "                f' {width:.4f}', va='center', ha='left', fontsize=10)\n",
        "\n",
        "    # Adjust the X-axis range to make space for the labels.\n",
        "    ax.set_xlim(right=ax.get_xlim()[1] * 1.2)\n",
        "\n",
        "    # --- Plot the inset donut chart ---\n",
        "    N_DONUT_FEATURES = 5\n",
        "    if len(df_importance) < N_DONUT_FEATURES:\n",
        "        N_DONUT_FEATURES = len(df_importance)\n",
        "\n",
        "    # Sort in descending order to get the top N features.\n",
        "    df_desc = df_importance.sort_values(by='Importance', ascending=False)\n",
        "    top_n_features = df_desc.head(N_DONUT_FEATURES)\n",
        "    donut_feature_names = top_n_features['Feature'].tolist()\n",
        "\n",
        "    # Proceed only if there are features with importance greater than 0.\n",
        "    if not top_n_features.empty and top_n_features['Importance'].sum() > 0:\n",
        "        # Calculate percentages for the donut chart slices.\n",
        "        total_donut_importance = top_n_features['Importance'].sum()\n",
        "        donut_percentages = top_n_features['Importance'] / total_donut_importance * 100\n",
        "\n",
        "        # Add an inset axis for the donut chart.\n",
        "        ax_inset = fig.add_axes([0.45, 0.15, 0.28, 0.28]) # [left, bottom, width, height]\n",
        "\n",
        "        colors = matplotlib.colormaps['tab10'].colors\n",
        "        wedges, _ = ax_inset.pie(\n",
        "            donut_percentages,\n",
        "            colors=colors[:len(top_n_features)], startangle=90, counterclock=False,\n",
        "            wedgeprops=dict(width=0.45, edgecolor='w') # The 'width' property creates the donut hole.\n",
        "        )\n",
        "\n",
        "        # Add text to the center of the donut chart.\n",
        "        subset_importance_ratio = top_n_features['Importance'].sum() / df_importance['Importance'].sum()\n",
        "        ax_inset.text(0, 0, f'Top {N_DONUT_FEATURES} Features\\nAccount for\\n{subset_importance_ratio:.2%} of Total Importance',\n",
        "                      ha='center', va='center', fontsize=9, linespacing=1.4)\n",
        "\n",
        "        # Logic to add and position labels for the donut slices.\n",
        "        label_threshold = 3.0\n",
        "        y_text_offsets = {'left': 1.4, 'right': 1.4}\n",
        "        for i, p in enumerate(wedges):\n",
        "            percent = donut_percentages.iloc[i]\n",
        "            ang = (p.theta2 - p.theta1) / 2. + p.theta1\n",
        "            y = np.sin(np.deg2rad(ang))\n",
        "            x = np.cos(np.deg2rad(ang))\n",
        "\n",
        "            # For small slices, draw a line to an external label.\n",
        "            if percent < label_threshold and percent > 0:\n",
        "                side = 'right' if x > 0 else 'left'\n",
        "                y_pos = y_text_offsets[side]\n",
        "                y_text_offsets[side] += -0.2 if y > 0 else 0.2\n",
        "                connectionstyle = f\"angle,angleA=0,angleB={ang}\"\n",
        "                ax_inset.annotate(f'{percent:.1f}%', xy=(x, y), xytext=(1.2 * np.sign(x), y_pos),\n",
        "                                  fontsize=9, ha='center',\n",
        "                                  arrowprops=dict(arrowstyle=\"-\", connectionstyle=connectionstyle, relpos=(0.5, 0.5)))\n",
        "            # For larger slices, place the label inside.\n",
        "            elif percent > 0:\n",
        "                ax_inset.text(x * 0.8, y * 0.8, f'{percent:.1f}%', ha='center', va='center', fontsize=9,\n",
        "                              fontweight='bold', color='white')\n",
        "\n",
        "        # Add a legend for the donut chart outside the pie area.\n",
        "        ax_inset.legend(wedges, donut_feature_names,\n",
        "                        loc=\"center left\", bbox_to_anchor=(1.2, 0.5),\n",
        "                        frameon=False, fontsize=11)\n",
        "\n",
        "    # Save the figure to the specified path with high resolution.\n",
        "    plt.savefig(save_path, dpi=720, bbox_inches='tight')\n",
        "    plt.close(fig)"
      ],
      "metadata": {
        "id": "U8wfMvvC1fNO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('----------------------------------------Calculating and Plotting GBDT Native Feature Importance-----------------------------------------')\n",
        "importances = loaded_model.feature_importances_\n",
        "gbdt_importance_df = pd.DataFrame({'Feature': feature_names_from_df, 'Importance': importances})\n",
        "save_path_gbdt = os.path.join(results_plot_save_dir, 'GBDT_Feature_Importance_Combined_Plot_final.png')\n",
        "plot_importance_combined(gbdt_importance_df, 'Feature Importance Calculated by GBDT Model', save_path_gbdt, bar_color='dodgerblue')"
      ],
      "metadata": {
        "id": "xK3AFnKU1lFG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 2. Permutation Importance Plot ---\n",
        "print(\"----------------------------------------Calculating and plotting Permutation Importance-----------------------------------------\")\n",
        "scores = defaultdict(list)\n",
        "for feat_name in feature_names_from_df:\n",
        "    X_t = X_test_scaled_df.copy()\n",
        "    X_t[feat_name] = np.random.permutation(X_t[feat_name].values)\n",
        "    shuff_acc = r2_score(y_test, loaded_model.predict(X_t))\n",
        "    scores[feat_name].append((test_r2 - shuff_acc) / test_r2 if test_r2 > 1e-6 else test_r2 - shuff_acc)\n",
        "\n",
        "sorted_scores = sorted([(np.mean(score_list), feat) for feat, score_list in scores.items()], reverse=True)\n",
        "\n",
        "perm_feature_names = [feat for _, feat in sorted_scores]\n",
        "perm_feature_scores = [score for score, _ in sorted_scores]\n",
        "\n",
        "perm_importance_df = pd.DataFrame({'Feature': perm_feature_names, 'Importance': perm_feature_scores})\n",
        "\n",
        "save_path_perm = os.path.join(results_plot_save_dir, 'GBDT_Feature_Importance_Permutation_final.png')\n",
        "\n",
        "plot_importance_combined(perm_importance_df, 'Feature Importance (Permutation Importance for GBDT)', save_path_perm, bar_color='lightcoral')"
      ],
      "metadata": {
        "id": "XUgXUXDm1n4m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('----------------------------------------Plotting Residual Analysis-----------------------------------------')\n",
        "train_residuals = y_train - y_train_pred\n",
        "test_residuals = y_test - y_test_pred\n",
        "\n",
        "def plot_residuals_styled(residuals, y_pred, save_path, title):\n",
        "    plt.style.use('seaborn-v0_8-whitegrid')\n",
        "    plt.rc(\"font\", family='Arial')\n",
        "    fig, ax = plt.subplots(figsize=(10, 8))\n",
        "    sd_residuals = np.std(residuals)\n",
        "    is_outlier = np.abs(residuals) > 2 * sd_residuals\n",
        "    num_outliers = np.sum(is_outlier)\n",
        "    print(f\"In dataset '{title}', found {num_outliers} outliers (residual > 2 S.D.).\")\n",
        "    sd_label = f'S.D. (±{sd_residuals:.2f})'\n",
        "    ax.axhspan(-sd_residuals, sd_residuals, color='yellow', alpha=0.3, label=sd_label)\n",
        "    ax.scatter(y_pred[~is_outlier], residuals[~is_outlier], alpha=0.6, c='green',\n",
        "               edgecolors='k', linewidth=0.5, s=50, label='Normal')\n",
        "    ax.scatter(y_pred[is_outlier], residuals[is_outlier], alpha=0.8, c='red',\n",
        "               edgecolors='k', linewidth=0.5, s=70, label='Outlier (> 2 S.D.)')\n",
        "    ax.axhline(0, color='black', linestyle='--', linewidth=1.5)\n",
        "    ax.set_title(title, fontsize=16, weight='bold')\n",
        "    ax.set_xlabel('Predicted Value', fontsize=14)\n",
        "    ax.set_ylabel('Residual (True - Predicted)', fontsize=14)\n",
        "    for spine in ax.spines.values():\n",
        "        spine.set_visible(True)\n",
        "        spine.set_color('black')\n",
        "        spine.set_linewidth(1)\n",
        "    ax.legend(loc='upper right', fontsize=12)\n",
        "    y_max = np.max(np.abs(residuals)) * 1.2\n",
        "    ax.set_ylim(-y_max, y_max)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
        "    plt.close()\n",
        "\n",
        "train_res_path = os.path.join(results_plot_save_dir, 'GBDT_train_residual_analysis_final.png')\n",
        "test_res_path = os.path.join(results_plot_save_dir, 'GBDT_validation_residual_analysis_final.png')\n",
        "plot_residuals_styled(train_residuals, y_train_pred, train_res_path, 'GBDT Train Residual Analysis')\n",
        "plot_residuals_styled(test_residuals, y_test_pred, test_res_path, 'GBDT Validation Residual Analysis')"
      ],
      "metadata": {
        "id": "7ObslZ0F1txy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Complete Section for PDP and ICE Plotting ---\n",
        "print('------------------------ Starting PDP and ICE Plotting ------------------------')\n",
        "pdp_ice_save_dir = os.path.join(results_plot_save_dir, 'GBDT_PDP_ICE_Plots_final')\n",
        "os.makedirs(pdp_ice_save_dir, exist_ok=True)\n",
        "pdp_2way_save_dir = os.path.join(pdp_ice_save_dir, '2Way_PDP_All_Combinations')\n",
        "os.makedirs(pdp_2way_save_dir, exist_ok=True)\n",
        "pdp_3d_save_dir = os.path.join(pdp_ice_save_dir, '3D_PDP_All_Combinations')\n",
        "os.makedirs(pdp_3d_save_dir, exist_ok=True)\n",
        "\n",
        "n_top_features_for_pdp = 6\n",
        "if n_top_features_for_pdp > len(feature_names_from_df):\n",
        "    n_top_features_for_pdp = len(feature_names_from_df)\n",
        "top_features_pdp_names = gbdt_importance_df['Feature'].tolist()[:n_top_features_for_pdp]\n",
        "plt.style.use('seaborn-v0_8-whitegrid')\n",
        "plt.rc(\"font\", family='Arial')\n",
        "\n",
        "print(\"\\nStart plotting univariate PDP (individual)...\")\n",
        "for feature_name_str in top_features_pdp_names:\n",
        "    try:\n",
        "        fig_pdp, ax_pdp = plt.subplots(figsize=(8, 6))\n",
        "        PartialDependenceDisplay.from_estimator(loaded_model, X_train_scaled_df, [feature_name_str], kind='average',\n",
        "                                                ax=ax_pdp)\n",
        "        ax_pdp.set_title(f'Partial Dependence Plot (PDP)\\nFeature: {feature_name_str}', fontsize=14)\n",
        "        ax_pdp.set_xlabel(f'{feature_name_str} (Standardized Value)', fontsize=12)\n",
        "        ax_pdp.set_ylabel('Dependence on Predicted Value (PDP)', fontsize=12)\n",
        "        plt.savefig(os.path.join(pdp_ice_save_dir, f'GBDT_PDP_single_{feature_name_str}.png'), dpi=300,\n",
        "                    bbox_inches='tight')\n",
        "        plt.close(fig_pdp)\n",
        "    except Exception as e:\n",
        "        print(f\"Error plotting PDP for {feature_name_str}: {e}\")\n",
        "\n",
        "# --- Method 2: Manually compute PDP with shaded 95% confidence interval ---\n",
        "print(\"\\nStart plotting univariate PDP (with shaded confidence interval)...\")\n",
        "for feature_name_str in top_features_pdp_names:\n",
        "    try:\n",
        "        # 1. Use partial_dependence to compute ICE values for each sample\n",
        "        pd_results = partial_dependence(\n",
        "            loaded_model,\n",
        "            X_train_scaled_df,\n",
        "            [feature_name_str],\n",
        "            kind='individual'  # must be 'individual' to retrieve each sample's ICE data\n",
        "        )\n",
        "\n",
        "        # 2. Compute average (PDP) and standard deviation\n",
        "        ice_lines = pd_results['individual'][0]\n",
        "        pdp_line = np.mean(ice_lines, axis=0)\n",
        "        pdp_std = np.std(ice_lines, axis=0)\n",
        "\n",
        "        # 3. Get feature value grid\n",
        "        grid_values = pd_results['values'][0]\n",
        "\n",
        "        # 4. Plot\n",
        "        fig, ax = plt.subplots(figsize=(8, 6))\n",
        "\n",
        "        # PDP average effect line\n",
        "        ax.plot(grid_values, pdp_line, color='red', linestyle='--', linewidth=2, label='Average Effect (PDP)')\n",
        "\n",
        "        # 95% Confidence Interval (mean ± 1.96 * std)\n",
        "        ax.fill_between(\n",
        "            grid_values,\n",
        "            pdp_line - 1.96 * pdp_std,\n",
        "            pdp_line + 1.96 * pdp_std,\n",
        "            color='skyblue',\n",
        "            alpha=0.3,\n",
        "            label='95% Confidence Interval'\n",
        "        )\n",
        "\n",
        "        ax.set_title(f'Partial Dependence Plot (PDP) with 95% CI\\nFeature: {feature_name_str}', fontsize=14)\n",
        "        ax.set_xlabel(f'{feature_name_str} (Standardized Value)', fontsize=12)\n",
        "        ax.set_ylabel('Partial Dependence on Predicted Value', fontsize=12)\n",
        "        ax.legend()\n",
        "\n",
        "        save_path = os.path.join(pdp_ice_save_dir, f'GBDT_PDP_with_Shaded_CI_{feature_name_str}.png')\n",
        "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
        "        plt.close(fig)\n",
        "    except Exception as e:\n",
        "        print(f\"Error plotting PDP (with shaded CI) for {feature_name_str}: {e}\")\n",
        "\n",
        "print(\"\\nStart plotting univariate ICE (individual)...\")\n",
        "for feature_name_str in top_features_pdp_names:\n",
        "    try:\n",
        "        fig_ice, ax_ice = plt.subplots(figsize=(8, 6))\n",
        "        PartialDependenceDisplay.from_estimator(loaded_model, X_train_scaled_df, [feature_name_str], kind='individual',\n",
        "                                                centered=True, ice_lines_kw={\"color\": \"tab:blue\", \"alpha\": 0.1},\n",
        "                                                pd_line_kw={'color': 'red', 'linestyle': '--', 'linewidth': 2},\n",
        "                                                ax=ax_ice)\n",
        "        ax_ice.set_title(f'Individual Conditional Expectation (ICE)\\nFeature: {feature_name_str}', fontsize=14)\n",
        "        ax_ice.set_xlabel(f'{feature_name_str} (Standardized Value)', fontsize=12)\n",
        "        ax_ice.set_ylabel('Dependence on Predicted Value (ICE)', fontsize=12)\n",
        "        plt.savefig(os.path.join(pdp_ice_save_dir, f'GBDT_ICE_single_{feature_name_str}.png'), dpi=300,\n",
        "                    bbox_inches='tight')\n",
        "        plt.close(fig_ice)\n",
        "    except Exception as e:\n",
        "        print(f\"Error plotting ICE for {feature_name_str}: {e}\")\n",
        "\n",
        "print(\"\\nStart plotting combined PDP and ICE...\")\n",
        "for feature_name_str in top_features_pdp_names:\n",
        "    try:\n",
        "        fig_comb, ax_comb = plt.subplots(figsize=(8, 6))\n",
        "        PartialDependenceDisplay.from_estimator(loaded_model, X_train_scaled_df, [feature_name_str], kind='both',\n",
        "                                                centered=True,\n",
        "                                                pd_line_kw={\"color\": \"tab:red\", \"linestyle\": \"--\", \"linewidth\": 2.5},\n",
        "                                                ice_lines_kw={\"color\": \"tab:blue\", \"alpha\": 0.1}, ax=ax_comb)\n",
        "        ax_comb.set_title(f'Combined PDP and ICE\\nFeature: {feature_name_str}', fontsize=14)\n",
        "        ax_comb.set_xlabel(f'{feature_name_str} (Standardized Value)', fontsize=12)\n",
        "        ax_comb.set_ylabel('Dependence on Predicted Value', fontsize=12)\n",
        "        plt.savefig(os.path.join(pdp_ice_save_dir, f'GBDT_PDP_ICE_combined_{feature_name_str}.png'), dpi=300,\n",
        "                    bbox_inches='tight')\n",
        "        plt.close(fig_comb)\n",
        "    except Exception as e:\n",
        "        print(f\"Error plotting PDP/ICE for {feature_name_str}: {e}\")\n",
        "\n",
        "print(\"\\nStart plotting bivariate (2D) PDP...\")\n",
        "if len(top_features_pdp_names) >= 2:\n",
        "    for feat1_name, feat2_name in combinations(top_features_pdp_names, 2):\n",
        "        try:\n",
        "            fig_2d_pdp, ax_2d_pdp = plt.subplots(figsize=(8, 7))\n",
        "            PartialDependenceDisplay.from_estimator(loaded_model, X_train_scaled_df, [(feat1_name, feat2_name)],\n",
        "                                                    ax=ax_2d_pdp)\n",
        "            ax_2d_pdp.set_title(f'2D PDP: {feat1_name} vs {feat2_name}', fontsize=16)\n",
        "            plt.tight_layout()\n",
        "            plt.savefig(os.path.join(pdp_2way_save_dir, f'GBDT_PDP_2D_{feat1_name}_{feat2_name}.png'), dpi=300)\n",
        "            plt.close(fig_2d_pdp)\n",
        "        except Exception as e:\n",
        "            print(f\"Error plotting 2D PDP for {feat1_name} & {feat2_name}: {e}\")\n",
        "\n",
        "# --- [Fixed] Section for 3D PDP Plotting ---\n",
        "print(\"\\nStart plotting bivariate (3D) PDP...\")\n",
        "if len(top_features_pdp_names) >= 2:\n",
        "    # Get feature list for index lookup\n",
        "    feature_list = X_train_scaled_df.columns.tolist()\n",
        "\n",
        "    for feat1_name, feat2_name in combinations(top_features_pdp_names, 2):\n",
        "        try:\n",
        "            # === Fix: Convert feature names to integer indices ===\n",
        "            feat1_idx = feature_list.index(feat1_name)\n",
        "            feat2_idx = feature_list.index(feat2_name)\n",
        "\n",
        "            features_to_plot = [(feat1_idx, feat2_idx)]\n",
        "\n",
        "            pd_results = partial_dependence(loaded_model, X_train_scaled_df, features=features_to_plot,\n",
        "                                            kind='average', grid_resolution=20)\n",
        "\n",
        "            XX, YY = np.meshgrid(pd_results['values'][0], pd_results['values'][1])\n",
        "            ZZ = pd_results['average'][0].T\n",
        "\n",
        "            fig = plt.figure(figsize=(12, 9))\n",
        "            ax = fig.add_subplot(111, projection='3d')\n",
        "            surf = ax.plot_surface(XX, YY, ZZ, cmap='viridis', edgecolor='none', antialiased=True)\n",
        "            fig.colorbar(surf, shrink=0.5, aspect=10, label='Partial Dependence Value')\n",
        "\n",
        "            ax.set_xlabel(f'{feat1_name} (Standardized Value)', fontsize=10, labelpad=10)\n",
        "            ax.set_ylabel(f'{feat2_name} (Standardized Value)', fontsize=10, labelpad=10)\n",
        "            ax.set_zlabel('Dependence on Predicted Value (PDP)', fontsize=10, labelpad=10)\n",
        "            ax.set_title(f'3D Partial Dependence Plot (3D PDP)\\n{feat1_name} vs {feat2_name}', fontsize=14)\n",
        "            ax.view_init(elev=20, azim=135)\n",
        "\n",
        "            save_path = os.path.join(pdp_3d_save_dir, f'GBDT_PDP_3D_{feat1_name}_{feat2_name}.png')\n",
        "            plt.savefig(save_path, dpi=300)\n",
        "            plt.close(fig)\n",
        "            print(f\"Successfully plotted 3D PDP for {feat1_name} & {feat2_name}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error plotting 3D PDP for {feat1_name} & {feat2_name}: {e}\")"
      ],
      "metadata": {
        "id": "PHulDCey1y5S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('------------------------ Start SHAP Analysis ------------------------')\n",
        "shap_save_dir = os.path.join(results_plot_save_dir, 'GBDT_SHAP_Plots_final')\n",
        "os.makedirs(shap_save_dir, exist_ok=True)\n",
        "explainer = shap.TreeExplainer(loaded_model)\n",
        "shap_values = explainer(X_test_scaled_df)\n",
        "\n",
        "# --- 3. SHAP Summary Bar Plot ---\n",
        "print(\"\\nDrawing SHAP Summary Plot (Bar Chart)...\")\n",
        "# Extract mean absolute values from shap_values as feature importance\n",
        "shap_importance_vals = np.abs(shap_values.values).mean(axis=0)\n",
        "shap_importance_df = pd.DataFrame({'Feature': X_test_scaled_df.columns, 'Importance': shap_importance_vals})\n",
        "save_path_shap = os.path.join(shap_save_dir, 'GBDT_SHAP_Feature_Importance_Combined_final.png')\n",
        "plot_importance_combined(shap_importance_df, 'SHAP Feature Importance (Mean Absolute SHAP Value)', save_path_shap, bar_color='#007bff')\n",
        "\n",
        "\n",
        "print(\"Drawing SHAP Summary Plot (Scatter Distribution)...\")\n",
        "shap.summary_plot(shap_values, X_test_scaled_df, show=False)\n",
        "plt.title('SHAP Feature Impact Overview (Scatter Distribution)', fontsize=16)\n",
        "plt.tight_layout()\n",
        "plt.savefig(os.path.join(shap_save_dir, 'GBDT_SHAP_summary_scatter.png'), dpi=300, bbox_inches='tight')\n",
        "plt.close()\n",
        "\n",
        "print(\"Drawing SHAP Dependence Plots...\")\n",
        "shap_dependence_save_dir = os.path.join(shap_save_dir, 'Dependence_Plots')\n",
        "os.makedirs(shap_dependence_save_dir, exist_ok=True)\n",
        "for feature_name in top_features_pdp_names:\n",
        "    shap.dependence_plot(feature_name, shap_values.values, X_test_scaled_df, interaction_index=\"auto\", show=False)\n",
        "    plt.gcf().suptitle(f'SHAP Dependence Plot: {feature_name}', fontsize=16)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(shap_dependence_save_dir, f'GBDT_SHAP_dependence_{feature_name}.png'), dpi=300,\n",
        "                bbox_inches='tight')\n",
        "    plt.close()\n",
        "\n",
        "# --- SHAP Waterfall Plot ---\n",
        "print(\"Drawing SHAP Waterfall Plot (for the first test sample)...\")\n",
        "plt.figure()  # Create a new figure object\n",
        "shap.plots.waterfall(shap_values[0], max_display=15, show=False)\n",
        "plt.title('SHAP Waterfall Plot (Test Sample 0)', fontsize=16)\n",
        "plt.tight_layout()\n",
        "plt.savefig(os.path.join(shap_save_dir, 'GBDT_SHAP_waterfall_sample_0.png'), dpi=300, bbox_inches='tight')\n",
        "plt.close()\n",
        "print('---------------------------------------- SHAP Analysis Completed -----------------------------------------')"
      ],
      "metadata": {
        "id": "LW_tdBGk1329"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('---------------------------------------- Starting ALE Analysis -----------------------------------------')\n",
        "ale_save_dir = os.path.join(results_plot_save_dir, 'GBDT_ALE_Plots_final')\n",
        "os.makedirs(ale_save_dir, exist_ok=True)\n",
        "print(f\"ALE plots will be saved to: {ale_save_dir}\")\n",
        "top_features_ale_names = top_features_pdp_names\n",
        "\n",
        "# --- 1D ALE Plotting (Colored) ---\n",
        "print(f\"\\nStarting 1D ALE plots for the top {len(top_features_ale_names)} features...\")\n",
        "# Create a color list to assign different colors for each feature plot\n",
        "colors = plt.cm.viridis(np.linspace(0, 0.85, len(top_features_ale_names)))\n",
        "\n",
        "for i, feature_name in enumerate(top_features_ale_names):\n",
        "    try:\n",
        "        # Use PyALE to compute, which automatically creates a plot\n",
        "        ale_eff = ale(X=X_train_scaled_df, model=loaded_model, feature=[feature_name],\n",
        "                      feature_type='continuous', grid_size=50, include_CI=True, C=0.95)\n",
        "\n",
        "        # Get current figure and axis\n",
        "        fig, ax = plt.gcf(), plt.gca()\n",
        "        current_color = colors[i]\n",
        "\n",
        "        # Modify line color and width\n",
        "        if ax.lines:\n",
        "            ax.lines[0].set_color(current_color)\n",
        "            ax.lines[0].set_linewidth(2.5)\n",
        "\n",
        "        # Modify confidence interval fill color and transparency\n",
        "        if ax.collections:\n",
        "            ax.collections[0].set_facecolor(current_color)\n",
        "            ax.collections[0].set_alpha(0.2)\n",
        "\n",
        "        ax.set_title(f'Cumulative Local Effects (ALE) - Feature: {feature_name}', fontsize=16)\n",
        "        ax.set_xlabel(f'{feature_name} (standardized value)', fontsize=12)\n",
        "        ax.set_ylabel('ALE (effect on prediction)', fontsize=12)\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(os.path.join(ale_save_dir, f'GBDT_ALE_1D_{feature_name}.png'), dpi=300, bbox_inches='tight')\n",
        "        plt.close(fig)\n",
        "    except Exception as e:\n",
        "        print(f\"Error plotting 1D ALE for {feature_name}: {e}\")\n",
        "        if plt.get_fignums():\n",
        "            plt.close('all')\n",
        "\n",
        "# --- 2D ALE Plotting (Colored Heatmap) ---\n",
        "print(f\"\\nStarting 2D ALE plots for the most important feature pairs...\")\n",
        "if len(top_features_ale_names) >= 2:\n",
        "    for feat1_name, feat2_name in combinations(top_features_ale_names, 2):\n",
        "        try:\n",
        "            # Compute ALE effect only, do not plot automatically\n",
        "            ale_eff_2d = ale(X=X_train_scaled_df, model=loaded_model, feature=[feat1_name, feat2_name],\n",
        "                             feature_type='continuous', grid_size=30, plot=False)\n",
        "\n",
        "            # Manually create a colored heatmap\n",
        "            fig, ax = plt.subplots(figsize=(8, 7))\n",
        "            im = ax.pcolormesh(ale_eff_2d.index, ale_eff_2d.columns, ale_eff_2d.values.T, cmap='viridis',\n",
        "                               shading='auto')\n",
        "            fig.colorbar(im, ax=ax, label='ALE (effect on prediction)')\n",
        "\n",
        "            ax.set_title(f'2D ALE: {feat1_name} vs {feat2_name}', fontsize=16)\n",
        "            ax.set_xlabel(f'{feat1_name} (standardized value)', fontsize=12)\n",
        "            ax.set_ylabel(f'{feat2_name} (standardized value)', fontsize=12)\n",
        "            plt.tight_layout()\n",
        "            plt.savefig(os.path.join(ale_save_dir, f'GBDT_ALE_2D_{feat1_name}_vs_{feat2_name}.png'), dpi=300,\n",
        "                        bbox_inches='tight')\n",
        "            plt.close(fig)\n",
        "        except Exception as e:\n",
        "            print(f\"Error plotting 2D ALE for {feat1_name} & {feat2_name}: {e}\")\n",
        "            if plt.get_fignums():\n",
        "                plt.close('all')\n",
        "\n",
        "print('---------------------------------------- ALE Analysis Completed -----------------------------------------')\n",
        "print('---------------------------------------- Script Execution Finished -----------------------------------------')"
      ],
      "metadata": {
        "id": "agwk_lAi1-lt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
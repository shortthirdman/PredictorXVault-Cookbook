{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c7a90d46-a0aa-401f-9c10-c3d176961883",
   "metadata": {},
   "source": [
    "## Lasso and Elastic Net Regressions\n",
    "\n",
    "> Roping in key features with coordinate descent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12e0f69e-18c2-4249-a96a-ee4631133d3e",
   "metadata": {},
   "source": [
    "#### Lasso Regression\n",
    "\n",
    "**_LASSO (Least Absolute Shrinkage and Selection Operator)_** is a variation of Linear Regression that adds a penalty to the model. It uses a linear equation to predict numbers, just like Linear Regression. However, Lasso also has a way to reduce the importance of certain factors to zero, which makes it useful for two main tasks: making predictions and identifying the most important features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1b08ab5-6e78-4d8f-ae06-91a5dd2e7a2d",
   "metadata": {},
   "source": [
    "#### Elastic Net Regression\n",
    "\n",
    "**_Elastic Net Regression_** is a mix of Ridge and Lasso Regression that combines their penalty terms. The name “Elastic Net” comes from physics: just like an elastic net can stretch and still keep its shape, this method adapts to data while maintaining structure.\n",
    "\n",
    "The model balances three goals: minimizing prediction errors, keeping the size of coefficients small (like Lasso), and preventing any coefficient from becoming too large (like Ridge). To use the model, you input your data’s feature values into the linear equation, just like in standard Linear Regression.\n",
    "\n",
    "The main advantage of Elastic Net is that when features are related, it tends to keep or remove them as a group instead of randomly picking one feature from the group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "afe40fcf-b35f-49e9-ae19-6cf5d66cfa5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "519f5864-477a-4459-9718-faba61a393fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "# Create dataset\n",
    "data = {\n",
    "    'Outlook': ['sunny', 'sunny', 'overcast', 'rain', 'rain', 'rain', 'overcast', 'sunny', 'sunny', \n",
    "                'rain', 'sunny', 'overcast', 'overcast', 'rain', 'sunny', 'overcast', 'rain', 'sunny', \n",
    "                'sunny', 'rain', 'overcast', 'rain', 'sunny', 'overcast', 'sunny', 'overcast', 'rain', 'overcast'],\n",
    "    'Temperature': [85, 80, 83, 70, 68, 65, 64, 72, 69, 75, 75, 72, 81, 71, 81, 74, 76, 78, 82, \n",
    "                   67, 85, 73, 88, 77, 79, 80, 66, 84],\n",
    "    'Humidity': [85, 90, 78, 96, 80, 70, 65, 95, 70, 80, 70, 90, 75, 80, 88, 92, 85, 75, 92, \n",
    "                 90, 85, 88, 65, 70, 60, 95, 70, 78],\n",
    "    'Wind': [False, True, False, False, False, True, True, False, False, False, True, True, False, \n",
    "             True, True, False, False, True, False, True, True, False, True, False, False, True, False, False],\n",
    "    'Num_Players': [52, 39, 43, 37, 28, 19, 43, 47, 56, 33, 49, 23, 42, 13, 33, 29, 25, 51, 41, \n",
    "                    14, 34, 29, 49, 36, 57, 21, 23, 41]\n",
    "}\n",
    "\n",
    "# Process data\n",
    "df = pd.get_dummies(pd.DataFrame(data), columns=['Outlook'])\n",
    "df['Wind'] = df['Wind'].astype(int)\n",
    "\n",
    "# Split data\n",
    "X, y = df.drop(columns='Num_Players'), df['Num_Players']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.5, shuffle=False)\n",
    "\n",
    "# Scale numerical features\n",
    "numerical_cols = ['Temperature', 'Humidity']\n",
    "ct = ColumnTransformer([('scaler', StandardScaler(), numerical_cols)], remainder='passthrough')\n",
    "\n",
    "# Transform data\n",
    "X_train_scaled = pd.DataFrame(\n",
    "    ct.fit_transform(X_train),\n",
    "    columns=numerical_cols + [col for col in X_train.columns if col not in numerical_cols],\n",
    "    index=X_train.index\n",
    ")\n",
    "\n",
    "X_test_scaled = pd.DataFrame(\n",
    "    ct.transform(X_test),\n",
    "    columns=X_train_scaled.columns,\n",
    "    index=X_test.index\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1cc0ebe-496e-438e-93f2-35daaaf25a7c",
   "metadata": {},
   "source": [
    "#### Main Mechanism\n",
    "\n",
    "**Lasso** and **Elastic Net** Regression predict numbers by making a straight line (or hyperplane) from the data, while controlling the size of coefficients in different ways:\n",
    "\n",
    "1. Both models find the best line by balancing prediction accuracy with coefficient control. They work to make the gaps between real and predicted values small, while keeping coefficients in check through penalty terms.\n",
    "\n",
    "2. In Lasso, the penalty (controlled by λ) can shrink coefficients to exactly zero, removing features entirely. Elastic Net combines two types of penalties: one that can remove features (like Lasso) and another that shrinks groups of related features together. The mix between these penalties is controlled by the l1_ratio (α).\n",
    "\n",
    "3. To predict a new answer, both models multiply each input by its coefficient (if not zero) and add them up, plus a starting number (intercept/bias). Elastic Net often keeps more features than Lasso but with smaller coefficients, especially when features are correlated.\n",
    "\n",
    "4. The strength of penalties affects how the models behave:\n",
    "- In Lasso, larger λ means more coefficients become zero\n",
    "- In Elastic Net, λ controls overall penalty strength, while α determines the balance between feature removal and coefficient shrinkage\n",
    "- When penalties are very small, both models act more like standard Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "950322ec-debe-4d32-a5eb-319a095291e2",
   "metadata": {},
   "source": [
    "#### Coordinate Descent for Lasso Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cba49f70-9437-4e80-8b0b-f6b7cd9e28d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients after one cycle:\n",
      "Temperature: 4.64\n",
      "Humidity   : -2.35\n",
      "Wind       : -5.19\n",
      "Outlook_overcast: 0.81\n",
      "Outlook_rain: -6.16\n",
      "Outlook_sunny: 11.55\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Initialize bias as mean of target values and coefficients to 0\n",
    "bias = np.mean(y_train)\n",
    "beta = np.zeros(X_train_scaled.shape[1])\n",
    "lambda_param = 1\n",
    "\n",
    "# One cycle through all features\n",
    "for j, feature in enumerate(X_train_scaled.columns):\n",
    "    # Get current feature values\n",
    "    x_j = X_train_scaled.iloc[:, j].values\n",
    "\n",
    "    # Calculate prediction excluding the j-th feature\n",
    "    y_pred_no_j = bias + X_train_scaled.values @ beta - x_j * beta[j]\n",
    "\n",
    "    # Calculate partial residuals\n",
    "    residual_no_j = y_train.values - y_pred_no_j\n",
    "\n",
    "    # Calculate the dot product of x_j with itself (sum of squared feature values)\n",
    "    sum_squared_x_j = np.dot(x_j, x_j)\n",
    "\n",
    "    # Calculate temporary beta without regularization (raw update)\n",
    "    beta_old = beta[j]\n",
    "    beta_temp = beta_old + np.dot(x_j, residual_no_j) / sum_squared_x_j\n",
    "\n",
    "    # Apply soft thresholding for Lasso penalty\n",
    "    beta[j] = np.sign(beta_temp) * max(abs(beta_temp) - lambda_param / sum_squared_x_j, 0)\n",
    "\n",
    "# Print results\n",
    "print(\"Coefficients after one cycle:\")\n",
    "for feature, coef in zip(X_train_scaled.columns, beta):\n",
    "    print(f\"{feature:11}: {coef:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0ed0c544-ba78-45c9-9707-c118656366e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update bias (not penalized by lambda)\n",
    "y_pred = X_train_scaled.values @ beta  # only using coefficients, no bias\n",
    "residuals = y_train.values - y_pred\n",
    "bias = np.mean(residuals)  # this replaces the old bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "28ef6fb9-6c75-406c-9279-19d46020dd1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Coefficients after 1000 cycles:\n",
      "Bias term  : 41.24\n",
      "Temperature: 0.00\n",
      "Humidity   : -1.35\n",
      "Wind       : -7.91\n",
      "Outlook_overcast: -0.00\n",
      "Outlook_rain: -9.14\n",
      "Outlook_sunny: 7.97\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "# Fit Lasso from scikit-learn\n",
    "lasso = Lasso(alpha=1) # Default value is 1000 cycle\n",
    "lasso.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Print results\n",
    "print(\"\\nCoefficients after 1000 cycles:\")\n",
    "print(f\"Bias term  : {lasso.intercept_:.2f}\")\n",
    "for feature, coef in zip(X_train_scaled.columns, lasso.coef_):\n",
    "   print(f\"{feature:11}: {coef:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce5b0f84-604e-4e51-8bac-685e0c1845c3",
   "metadata": {},
   "source": [
    "#### Coordinate Descent for Elastic Net Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9944a8e9-2092-4073-98c4-87446b8f13d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients after one cycle:\n",
      "Temperature: 4.51\n",
      "Humidity   : -2.27\n",
      "Wind       : -4.89\n",
      "Outlook_overcast: 0.74\n",
      "Outlook_rain: -5.88\n",
      "Outlook_sunny: 10.51\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Initialize bias as mean of target values and coefficients to 0\n",
    "bias = np.mean(y_train)\n",
    "beta = np.zeros(X_train_scaled.shape[1])\n",
    "lambda_param = 1\n",
    "alpha = 0.5  # mixing parameter (0 for Ridge, 1 for Lasso)\n",
    "\n",
    "# One cycle through all features\n",
    "for j, feature in enumerate(X_train_scaled.columns):\n",
    "    # Get current feature values\n",
    "    x_j = X_train_scaled.iloc[:, j].values\n",
    "\n",
    "    # Calculate prediction excluding the j-th feature\n",
    "    y_pred_no_j = bias + X_train_scaled.values @ beta - x_j * beta[j]\n",
    "\n",
    "    # Calculate partial residuals\n",
    "    residual_no_j = y_train.values - y_pred_no_j\n",
    "\n",
    "    # Calculate the dot product of x_j with itself (sum of squared feature values)\n",
    "    sum_squared_x_j = np.dot(x_j, x_j)\n",
    "\n",
    "    # Calculate temporary beta without regularization (raw update)\n",
    "    beta_old = beta[j]\n",
    "    beta_temp = beta_old + np.dot(x_j, residual_no_j) / sum_squared_x_j\n",
    "\n",
    "    # Apply soft thresholding for Elastic Net penalty\n",
    "    l1_term = alpha * lambda_param / sum_squared_x_j     # L1 (Lasso) penalty term\n",
    "    l2_term = (1-alpha) * lambda_param / sum_squared_x_j # L2 (Ridge) penalty term\n",
    "    \n",
    "    # First apply L1 soft thresholding, then L2 scaling\n",
    "    beta[j] = (np.sign(beta_temp) * max(abs(beta_temp) - l1_term, 0)) / (1 + l2_term)\n",
    "\n",
    "# Print results\n",
    "print(\"Coefficients after one cycle:\")\n",
    "for feature, coef in zip(X_train_scaled.columns, beta):\n",
    "    print(f\"{feature:11}: {coef:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0782dba2-fb55-491a-80c9-6eff89ccf08d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update bias (not penalized by lambda)\n",
    "y_pred_with_updated_beta = X_train_scaled.values @ beta  # only using coefficients, no bias\n",
    "residuals_for_bias_update = y_train.values - y_pred_with_updated_beta\n",
    "new_bias = np.mean(y_train.values - y_pred_with_updated_beta)  # this replaces the old bias\n",
    "\n",
    "print(f\"Bias term  : {new_bias:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f7ae41fe-fa50-4807-a949-18ed90a12d48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Coefficients after 1000 cycles:\n",
      "Bias term  : 41.24\n",
      "Temperature: 0.00\n",
      "Humidity   : -1.35\n",
      "Wind       : -7.91\n",
      "Outlook_overcast: -0.00\n",
      "Outlook_rain: -9.14\n",
      "Outlook_sunny: 7.97\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import ElasticNet\n",
    "\n",
    "# Fit Lasso from scikit-learn\n",
    "elasticnet = Lasso(alpha=1) # Default value is 1000 cycle\n",
    "elasticnet.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Print results\n",
    "print(\"\\nCoefficients after 1000 cycles:\")\n",
    "print(f\"Bias term  : {elasticnet.intercept_:.2f}\")\n",
    "for feature, coef in zip(X_train_scaled.columns, elasticnet.coef_):\n",
    "   print(f\"{feature:11}: {coef:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4c3d0b3a-6785-435a-b36e-8d2956b65c6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Temperature  Humidity   Wind  Outlook_overcast  Outlook_rain  \\\n",
      "λ = 0                                                                 \n",
      "0.00         -1.07     -2.85 -13.49             -3.84        -16.56   \n",
      "0.25         -1.07     -2.85 -13.49             -3.84        -16.56   \n",
      "0.50         -1.07     -2.85 -13.49             -3.84        -16.56   \n",
      "0.75         -1.07     -2.85 -13.49             -3.84        -16.56   \n",
      "1.00         -1.07     -2.85 -13.49             -3.84        -16.56   \n",
      "\n",
      "       Outlook_sunny  Bias   RMSE  \n",
      "λ = 0                              \n",
      "0.00            7.36  47.6  7.026  \n",
      "0.25            7.36  47.6  7.026  \n",
      "0.50            7.36  47.6  7.026  \n",
      "0.75            7.36  47.6  7.026  \n",
      "1.00            7.36  47.6  7.026  \n",
      "          Temperature  Humidity   Wind  Outlook_overcast  Outlook_rain  \\\n",
      "λ = 0.01                                                                 \n",
      "0.00            -0.77     -2.81 -12.74              0.39        -11.68   \n",
      "0.25            -0.83     -2.82 -12.90              0.29        -11.91   \n",
      "0.50            -0.90     -2.83 -13.07              0.08        -12.26   \n",
      "0.75            -0.97     -2.83 -13.24              0.00        -12.49   \n",
      "1.00            -1.04     -2.84 -13.42             -0.00        -12.66   \n",
      "\n",
      "          Outlook_sunny   Bias   RMSE  \n",
      "λ = 0.01                               \n",
      "0.00              11.29  42.91  6.849  \n",
      "0.25              11.27  43.10  6.883  \n",
      "0.50              11.13  43.41  6.920  \n",
      "0.75              11.12  43.59  6.963  \n",
      "1.00              11.17  43.71  7.009  \n",
      "         Temperature  Humidity   Wind  Outlook_overcast  Outlook_rain  \\\n",
      "λ = 0.1                                                                 \n",
      "0.00            0.83     -2.48  -8.49             -0.08         -8.54   \n",
      "0.25            0.53     -2.53  -9.26             -0.00         -9.14   \n",
      "0.50            0.15     -2.59 -10.20              0.00         -9.93   \n",
      "0.75           -0.13     -2.69 -11.28              0.00        -10.78   \n",
      "1.00           -0.70     -2.76 -12.78             -0.00        -12.10   \n",
      "\n",
      "         Outlook_sunny   Bias   RMSE  \n",
      "λ = 0.1                               \n",
      "0.00              8.62  41.06  6.561  \n",
      "0.25              9.16  41.39  6.522  \n",
      "0.50              9.71  41.88  6.533  \n",
      "0.75             10.28  42.44  6.607  \n",
      "1.00             10.88  43.34  6.866  \n",
      "       Temperature  Humidity  Wind  Outlook_overcast  Outlook_rain  \\\n",
      "λ = 1                                                                \n",
      "0.00          1.83     -1.01 -2.07             -0.06         -2.71   \n",
      "0.25          1.85     -1.03 -2.35             -0.00         -3.08   \n",
      "0.50          1.79     -1.08 -2.82             -0.00         -3.71   \n",
      "0.75          1.48     -1.16 -3.86             -0.00         -4.94   \n",
      "1.00          0.00     -1.35 -7.91             -0.00         -9.14   \n",
      "\n",
      "       Outlook_sunny   Bias   RMSE  \n",
      "λ = 1                               \n",
      "0.00            2.77  38.31  9.374  \n",
      "0.25            3.18  38.40  9.168  \n",
      "0.50            3.83  38.59  8.838  \n",
      "0.75            5.06  39.04  8.260  \n",
      "1.00            7.97  41.24  7.203  \n",
      "        Temperature  Humidity  Wind  Outlook_overcast  Outlook_rain  \\\n",
      "λ = 10                                                                \n",
      "0.00           0.41     -0.12 -0.26              0.01         -0.39   \n",
      "0.25           0.25     -0.00 -0.03              0.00         -0.19   \n",
      "0.50           0.00     -0.00 -0.00              0.00         -0.00   \n",
      "0.75           0.00     -0.00 -0.00              0.00         -0.00   \n",
      "1.00           0.00     -0.00 -0.00              0.00         -0.00   \n",
      "\n",
      "        Outlook_sunny   Bias    RMSE  \n",
      "λ = 10                                \n",
      "0.00             0.38  37.54  11.714  \n",
      "0.25             0.18  37.44  11.985  \n",
      "0.50             0.00  37.43  12.199  \n",
      "0.75             0.00  37.43  12.199  \n",
      "1.00             0.00  37.43  12.199  \n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import root_mean_squared_error\n",
    "\n",
    "# Define parameters\n",
    "l1_ratios = [0, 0.25, 0.5, 0.75, 1]\n",
    "lambdas = [0, 0.01, 0.1, 1, 10]\n",
    "feature_names = X_train_scaled.columns\n",
    "\n",
    "# Create a dataframe for each lambda value\n",
    "for lambda_val in lambdas:\n",
    "    # Initialize list to store results\n",
    "    results = []\n",
    "    rmse_values = []\n",
    "    \n",
    "    # Fit ElasticNet for each l1_ratio\n",
    "    for l1_ratio in l1_ratios:\n",
    "        # Fit model\n",
    "        en = ElasticNet(alpha=lambda_val, l1_ratio=l1_ratio)\n",
    "        en.fit(X_train_scaled, y_train)\n",
    "        \n",
    "        # Calculate RMSE\n",
    "        y_pred = en.predict(X_test_scaled)\n",
    "        rmse = root_mean_squared_error(y_test, y_pred)\n",
    "        \n",
    "        # Store coefficients and RMSE\n",
    "        results.append(list(en.coef_.round(2)) + [round(en.intercept_,2),round(rmse,3)])\n",
    "    \n",
    "    # Create dataframe with RMSE column\n",
    "    columns = list(feature_names) + ['Bias','RMSE']\n",
    "    df = pd.DataFrame(results, index=l1_ratios, columns=columns)\n",
    "    df.index.name = f'λ = {lambda_val}'\n",
    "    \n",
    "    print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3cc6d7c2-ff3d-4e81-b999-66bfc618d5d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 6.8652\n",
      "\n",
      "Model Coefficients:\n",
      "Temperature  : -0.70\n",
      "Humidity     : -2.76\n",
      "sunny        : 10.88\n",
      "overcast     : 0.00\n",
      "rain         : -12.10\n",
      "Wind         : -12.78\n",
      "Intercept    : 43.34\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.metrics import root_mean_squared_error\n",
    "from sklearn.linear_model import Lasso  #, ElasticNet\n",
    "\n",
    "# Create dataset\n",
    "data = {\n",
    "    'Outlook': ['sunny', 'sunny', 'overcast', 'rain', 'rain', 'rain', 'overcast', 'sunny', 'sunny', \n",
    "                'rain', 'sunny', 'overcast', 'overcast', 'rain', 'sunny', 'overcast', 'rain', 'sunny', \n",
    "                'sunny', 'rain', 'overcast', 'rain', 'sunny', 'overcast', 'sunny', 'overcast', 'rain', 'overcast'],\n",
    "    'Temperature': [85, 80, 83, 70, 68, 65, 64, 72, 69, 75, 75, 72, 81, 71, 81, 74, 76, 78, 82, \n",
    "                   67, 85, 73, 88, 77, 79, 80, 66, 84],\n",
    "    'Humidity': [85, 90, 78, 96, 80, 70, 65, 95, 70, 80, 70, 90, 75, 80, 88, 92, 85, 75, 92, \n",
    "                 90, 85, 88, 65, 70, 60, 95, 70, 78],\n",
    "    'Wind': [False, True, False, False, False, True, True, False, False, False, True, True, False, \n",
    "             True, True, False, False, True, False, True, True, False, True, False, False, True, False, False],\n",
    "    'Num_Players': [52, 39, 43, 37, 28, 19, 43, 47, 56, 33, 49, 23, 42, 13, 33, 29, 25, 51, 41, \n",
    "                    14, 34, 29, 49, 36, 57, 21, 23, 41]\n",
    "}\n",
    "\n",
    "# Process data\n",
    "df = pd.get_dummies(pd.DataFrame(data), columns=['Outlook'], prefix='', prefix_sep='', dtype=int)\n",
    "df['Wind'] = df['Wind'].astype(int)\n",
    "df = df[['sunny','overcast','rain','Temperature','Humidity','Wind','Num_Players']]\n",
    "\n",
    "# Split data\n",
    "X, y = df.drop(columns='Num_Players'), df['Num_Players']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.5, shuffle=False)\n",
    "\n",
    "# Scale numerical features\n",
    "numerical_cols = ['Temperature', 'Humidity']\n",
    "ct = ColumnTransformer([('scaler', StandardScaler(), numerical_cols)], remainder='passthrough')\n",
    "\n",
    "# Transform data\n",
    "X_train_scaled = pd.DataFrame(\n",
    "    ct.fit_transform(X_train),\n",
    "    columns=numerical_cols + [col for col in X_train.columns if col not in numerical_cols],\n",
    "    index=X_train.index\n",
    ")\n",
    "X_test_scaled = pd.DataFrame(\n",
    "    ct.transform(X_test),\n",
    "    columns=X_train_scaled.columns,\n",
    "    index=X_test.index\n",
    ")\n",
    "\n",
    "# Initialize and train the model\n",
    "model = Lasso(alpha=0.1)  # Option 1: Lasso Regression (alpha is the regularization strength, equivalent to λ, uses coordinate descent)\n",
    "#model = ElasticNet(alpha=0.1, l1_ratio=0.5)  # Option 2: Elastic Net Regression (alpha is the overall regularization strength, and l1_ratio is the mix between L1 and L2, uses coordinate descent)\n",
    "\n",
    "# Fit the model\n",
    "model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "\n",
    "# Calculate and print RMSE\n",
    "rmse = root_mean_squared_error(y_test, y_pred)\n",
    "print(f\"RMSE: {rmse:.4f}\")\n",
    "\n",
    "# Additional information about the model\n",
    "print(\"\\nModel Coefficients:\")\n",
    "for feature, coef in zip(X_train_scaled.columns, model.coef_):\n",
    "    print(f\"{feature:13}: {coef:.2f}\")\n",
    "print(f\"Intercept    : {model.intercept_:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7398a3af-08d4-4639-b824-a80b718e3ed6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
